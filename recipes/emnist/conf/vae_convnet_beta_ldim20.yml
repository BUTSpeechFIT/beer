type: VAE
llh_type: beta
normalizing_flow:
  type: InverseAutoRegressive
  depth: 1
  iaf_block:
    activation: Tanh
    context_dim: 30
    data_dim: 30
    depth: 2
    width: 100
encoder:
  nnet_structure:
  - residual: no
    block_structure:
    - ReshapeLayer:shape=(-1,1,28,28)
    - Conv2d:in_channels=1;out_channels=16;kernel_size=(5,5);padding=1;stride=2
    - Softplus
    - Conv2d:in_channels=16;out_channels=32;kernel_size=(5,5);stride=2
    - Softplus
    - Conv2d:in_channels=32;out_channels=32;kernel_size=(5,5);stride=2
    - Softplus
    - ReshapeLayer:shape=(-1,32)
    - Linear:in_features=32;out_features=300
  prob_layer:
    type: NormalizingFlowLayer
    covariance: isotropic
    flow_params_dim: 30
    dim_in: 300
    dim_out: 30
decoder:
  nnet_structure:
  - residual: no
    block_structure:
    - Linear:in_features=30;out_features=300
    - Softplus
    - Linear:in_features=300;out_features=32
    - Softplus
    - ReshapeLayer:shape=(-1,32,1,1)
    - ConvTranspose2d:in_channels=32;out_channels=32;kernel_size=(5,5);stride=2
    - Softplus
    - ConvTranspose2d:in_channels=32;out_channels=16;kernel_size=(5,5);stride=2
    - Softplus
    - ConvTranspose2d:in_channels=16;out_channels=1;kernel_size=(5,5);padding=1;stride=2;output_padding=1
    - Softplus
    - ReshapeLayer:shape=(-1,<feadim>)
  prob_layer:
    type: BetaLayer
    dim_in: <feadim>
    dim_out: <feadim>
latent_model:
  type: Normal
  covariance: diagonal
  prior_strength: 1.
  noise_std: 0.1

