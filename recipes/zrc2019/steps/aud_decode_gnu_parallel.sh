#!/usr/bin/env bash


set -e

. path.sh


if [ $# -ne 5 ]; then
    echo "usage: <model> <uttids> <dataset> <njobs> <out-dir>"
    exit 1
fi

model=$1
uttids=$2
dataset=$3
njobs=$4
outdir=$(pwd)/$5

mkdir -p $outdir


function split_() {
    njobs=$1
    outdir=$2
    uttids=$3

    mkdir -p $outdir/{split,log,tmp}

    # randomize the data
    shuf_uttids=$(mktemp)
    shuf $uttids > $shuf_uttids
    #shuf $uttids --random-source=$uttids > $shuf_uttids #Â with a seed

    # split will give njobs files with a numerical name
    # used on submit_gnu_parallel, the number of files
    # generated by split_ is the same than the number
    # of processes that will be run in the computer
    split --numeric-suffixes=1 -a ${#njobs} \
        -n l/$njobs $shuf_uttids $outdir/split/

    rm -rf $shuf_utts

}
export -f split_;


function submit_gnu_parallel() {
    njobs=$1
    outdir=$2
    mdl=$3
    dataset=$4
    uttids=$5

    split_ $njobs $outdir $uttids

    ls -AF1 $outdir/split/* | \
        parallel --eta -j $njobs --tmpdir $outdir/tmp --files \
        "cat {} | beer hmm decode -u - --per-frame $mdl $dataset > $outdir/trans_{/.}.txt 2> $outdir/log/decode.txt"
}
export -f submit_gnu_parallel;

if [ ! -f $outdir/trans.txt ]; then
    # Accumulate the statistics in parallel.
    submit_gnu_parallel \
        "$njobs" \
        "$outdir" \
        "$model" \
        "$dataset" \
        "$uttids" || exit 1

    find $outdir/trans_*txt -exec cat {} > $outdir/trans.txt \;
else
    echo "Dataset already decoded. Skipping."
fi

