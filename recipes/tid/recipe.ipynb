{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07456140350877193"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NTOPICS = 40  # Number of topics {6, 40}\n",
    "#datapath = 'data/aud_subspace_mbn_4g_gamma_dirichlet_process_ldim40.trans'\n",
    "#datapath = 'data/aud_subspace_mbn_4g_gamma_dirichlet_process_ldim100.trans'\n",
    "#datapath = 'data/aud_mbn_4g_gamma_dirichlet_process.trans'\n",
    "datapath = 'data/aud_subspace_mfcc_8k_4g_gamma_dirichlet_process_ldim40.trans'\n",
    "\n",
    "with open(f'data/fisher_{NTOPICS}c_train.flist', 'r') as f:\n",
    "    train_docs = [line.strip() for line in f]\n",
    "    \n",
    "with open(f'data/fisher_{NTOPICS}c_test.flist', 'r') as f:\n",
    "    test_docs = [line.strip() for line in f]\n",
    "        \n",
    "with open('data/tID_tName.pkl', 'rb') as f:\n",
    "    topic_names = pickle.load(f)\n",
    "        \n",
    "document2topic = {}                                                            \n",
    "with open('data/fe_03_p1_calldata.tbl', 'r') as f:                                            \n",
    "    next(f) # skip the first line.                                             \n",
    "    for line in f:                                                             \n",
    "        tokens = line.strip().split(',')                                       \n",
    "        docid, raw_topic_name = tokens[0], tokens[2]\n",
    "        if raw_topic_name != '' and (docid in train_docs or docid in test_docs):      \n",
    "            topicid = int(raw_topic_name[3:])\n",
    "            document2topic[docid] = topic_names[topicid]\n",
    "\n",
    "document2topic_MIT = {}\n",
    "with open('data/fID_tID.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "for doc, topicid in labels.items():\n",
    "    document2topic_MIT[doc] = topic_names[topicid]\n",
    "    \n",
    "\n",
    "diff = 0\n",
    "for i, docid in enumerate(document2topic):\n",
    "    if document2topic[docid] != document2topic_MIT[docid]:\n",
    "        diff += 1\n",
    "diff / len(document2topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTOPICS = 40  # Number of topics {6, 40}\n",
    "#datapath = 'data/aud_subspace_mbn_4g_gamma_dirichlet_process_ldim40.trans'\n",
    "#datapath = 'data/aud_subspace_mbn_4g_gamma_dirichlet_process_ldim100.trans'\n",
    "#datapath = 'data/aud_mbn_4g_gamma_dirichlet_process.trans'\n",
    "datapath = 'data/aud_mfcc_8k_4g_gamma_dirichlet_process.trans'\n",
    "\n",
    "\n",
    "with open(f'data/fisher_{NTOPICS}c_train.flist', 'r') as f:\n",
    "    train_docs = [line.strip() for line in f]\n",
    "    \n",
    "with open(f'data/fisher_{NTOPICS}c_test.flist', 'r') as f:\n",
    "    test_docs = [line.strip() for line in f]\n",
    "    \n",
    "with open('data/tID_tName.pkl', 'rb') as f:\n",
    "    topic_names = pickle.load(f)\n",
    "        \n",
    "# Load all documents with their associated topic label.\n",
    "#document2topic = {}                                                            \n",
    "#with open('data/fe_03_p1_calldata.tbl', 'r') as f:                                            \n",
    "#    next(f) # skip the first line.                                             \n",
    "#    for line in f:                                                             \n",
    "#        tokens = line.strip().split(',')                                       \n",
    "#        docid, topicid = tokens[0], tokens[2]             \n",
    "#        if topicid != '' and (docid in train_docs or docid in test_docs):                              \n",
    "#            document2topic[docid] = topicid     \n",
    "            \n",
    "document2topic = {}\n",
    "with open('data/fID_tID.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "for doc, topicid in labels.items():\n",
    "    document2topic[doc] = topic_names[topicid]\n",
    "    \n",
    "        \n",
    "train_documents = []\n",
    "test_documents = []\n",
    "for docid in document2topic:\n",
    "    if docid in train_docs:\n",
    "        train_documents.append(docid)\n",
    "    elif docid in test_docs:\n",
    "        test_documents.append(docid)\n",
    "            \n",
    "# Build the reverse mapping topic -> documents\n",
    "topic2document = defaultdict(list)\n",
    "for doc, topic in document2topic.items():\n",
    "    topic2document[topic].append(doc)\n",
    "topics = sorted(list(topic2document.keys()))\n",
    "\n",
    "topic2document_train = defaultdict(list)\n",
    "for doc, topic in document2topic.items():\n",
    "    if doc in train_docs:\n",
    "        topic2document_train[topic].append(doc)\n",
    "\n",
    "topic2document_test = defaultdict(list)\n",
    "for doc, topic in document2topic.items():\n",
    "    if doc in test_docs:\n",
    "        topic2document_test[topic].append(doc)\n",
    "\n",
    "# Load the raw data\n",
    "rawdata = defaultdict(list)                                                     \n",
    "with open(datapath, 'r') as f:                                                  \n",
    "    for line in f:                                                          \n",
    "        tokens = line.strip().split()                                       \n",
    "        docid = tokens[0].replace('fe_03_', '')[:5]                        \n",
    "        rawdata[docid].append(' '.join(tokens[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Sports on TV',\n",
       " 2: 'Pets',\n",
       " 3: 'Life Partners',\n",
       " 4: 'Minimum Wage',\n",
       " 5: 'Comedy',\n",
       " 6: 'Perjury',\n",
       " 7: 'Money to Leave US',\n",
       " 8: 'Opening Own Business',\n",
       " 9: 'Time Travel',\n",
       " 10: 'Anonymous Benefactor',\n",
       " 11: 'US Public Schools',\n",
       " 12: 'Affirmative Action',\n",
       " 13: 'Movies',\n",
       " 14: 'Computer Games',\n",
       " 15: 'Current Events',\n",
       " 16: 'Hobbies',\n",
       " 17: 'Smoking',\n",
       " 18: 'Terrorism',\n",
       " 19: 'Televised Criminal Trials',\n",
       " 20: 'Drug Testing',\n",
       " 21: 'Family Values',\n",
       " 22: 'Censorship',\n",
       " 23: 'Health and Fitness',\n",
       " 24: 'September 11',\n",
       " 25: 'Strikes by Athletes',\n",
       " 26: 'Airport Security',\n",
       " 27: 'Middle East Issues',\n",
       " 28: 'Foreign Relations',\n",
       " 29: 'Education',\n",
       " 30: 'Family',\n",
       " 31: 'Corporate Conduct',\n",
       " 32: 'Outdoor Activities',\n",
       " 33: 'Friends',\n",
       " 34: 'Food',\n",
       " 35: 'Illness',\n",
       " 36: 'Personal Habits',\n",
       " 37: 'Reality TV',\n",
       " 38: 'Iraq Arms Inspections',\n",
       " 39: 'Holidays',\n",
       " 40: 'Bioterrorism'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection\n",
    "\n",
    "The vocabulary is selected per topic by choosing the $K$ n-gram $w$ with the highest following propbability:\n",
    "\n",
    "$$\n",
    "p(t | w ) = \\frac{f_{wt} + |T|p(t)}{f_w + |T|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = np.array([float(len(topic2document_train[topic])) for topic in topics]) \n",
    "p_t /= p_t.sum()\n",
    "ngram_order = 3\n",
    "\n",
    "class NGramCounter:\n",
    "    \n",
    "    def __init__(self, order=3, prior_count=0.):\n",
    "        self.order = order\n",
    "        self.counts = defaultdict(lambda: prior_count)\n",
    "        \n",
    "    def add(self, doc):\n",
    "        for utt in doc:\n",
    "            new_utt = '<s> ' * (self.order - 1)  + utt\n",
    "            tokens = new_utt.split()\n",
    "            for i in range(len(tokens) - self.order):\n",
    "                ngram = tuple(tokens[i:i+self.order])\n",
    "                self.counts[ngram] += 1\n",
    "                \n",
    "    def get_counts(self, vocab=None):\n",
    "        if vocab is None:\n",
    "            vocab = sorted(list(self.counts.keys()))\n",
    "        return [self.counts[word] for word in vocab]\n",
    "        \n",
    "# Evaluate the counts. \n",
    "global_counter = NGramCounter(order=ngram_order, prior_count=NTOPICS)\n",
    "topic_counters = {}\n",
    "for p_topic, topic in zip(p_t, topics):\n",
    "    topic_counters[topic] = NGramCounter(order=ngram_order, prior_count=NTOPICS * p_topic)\n",
    "    for doc in topic2document_train[topic]:\n",
    "        global_counter.add(rawdata[doc])\n",
    "        topic_counters[topic].add(rawdata[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ranked_ngrams = {}\n",
    "for topic in topics:\n",
    "    ranked_ngrams = []\n",
    "    for ngram in global_counter.counts:\n",
    "        score = topic_counters[topic].counts[ngram] / global_counter.counts[ngram]\n",
    "        ranked_ngrams.append((ngram, score))\n",
    "    topic_ranked_ngrams[topic]  = list(reversed(sorted(ranked_ngrams, key=lambda x: x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3976,\n",
       " [('<s>', 'sil', 'au55'),\n",
       "  ('<s>', 'sil', 'au74'),\n",
       "  ('<s>', 'sil', 'au86'),\n",
       "  ('au1', 'au100', 'au89'),\n",
       "  ('au1', 'au11', 'au86'),\n",
       "  ('au1', 'au17', 'au44'),\n",
       "  ('au1', 'au17', 'au8'),\n",
       "  ('au1', 'au22', 'au61'),\n",
       "  ('au1', 'au25', 'au2'),\n",
       "  ('au1', 'au33', 'au65'),\n",
       "  ('au1', 'au35', 'au16'),\n",
       "  ('au1', 'au35', 'au64'),\n",
       "  ('au1', 'au35', 'au84'),\n",
       "  ('au1', 'au37', 'au55'),\n",
       "  ('au1', 'au39', 'au59'),\n",
       "  ('au1', 'au55', 'au35'),\n",
       "  ('au1', 'au60', 'au1'),\n",
       "  ('au1', 'au61', 'au31'),\n",
       "  ('au1', 'au61', 'au65'),\n",
       "  ('au1', 'au61', 'au81'),\n",
       "  ('au1', 'au63', 'au65'),\n",
       "  ('au1', 'au70', 'au78'),\n",
       "  ('au1', 'au77', 'au19'),\n",
       "  ('au1', 'au81', 'au82'),\n",
       "  ('au1', 'au82', 'au19'),\n",
       "  ('au1', 'au85', 'au83'),\n",
       "  ('au1', 'au89', 'au23'),\n",
       "  ('au1', 'au92', 'au83'),\n",
       "  ('au1', 'au93', 'au36'),\n",
       "  ('au10', 'au12', 'au41'),\n",
       "  ('au10', 'au16', 'au73'),\n",
       "  ('au10', 'au17', 'au44'),\n",
       "  ('au10', 'au2', 'au14'),\n",
       "  ('au10', 'au2', 'au99'),\n",
       "  ('au10', 'au21', 'au1'),\n",
       "  ('au10', 'au22', 'au78'),\n",
       "  ('au10', 'au26', 'au1'),\n",
       "  ('au10', 'au26', 'au55'),\n",
       "  ('au10', 'au30', 'au36'),\n",
       "  ('au10', 'au31', 'au50'),\n",
       "  ('au10', 'au35', 'au44'),\n",
       "  ('au10', 'au41', 'au54'),\n",
       "  ('au10', 'au50', 'au60'),\n",
       "  ('au10', 'au50', 'au96'),\n",
       "  ('au10', 'au51', 'au17'),\n",
       "  ('au10', 'au57', 'au63'),\n",
       "  ('au10', 'au60', 'au18'),\n",
       "  ('au10', 'au60', 'au2'),\n",
       "  ('au10', 'au60', 'au21'),\n",
       "  ('au10', 'au60', 'au27'),\n",
       "  ('au10', 'au60', 'au81'),\n",
       "  ('au10', 'au61', 'au1'),\n",
       "  ('au10', 'au63', 'au18'),\n",
       "  ('au10', 'au63', 'au78'),\n",
       "  ('au10', 'au68', 'au54'),\n",
       "  ('au10', 'au68', 'au92'),\n",
       "  ('au10', 'au69', 'au86'),\n",
       "  ('au10', 'au70', 'au33'),\n",
       "  ('au10', 'au70', 'au39'),\n",
       "  ('au10', 'au70', 'au62'),\n",
       "  ('au10', 'au70', 'sil'),\n",
       "  ('au10', 'au72', 'au66'),\n",
       "  ('au10', 'au76', 'au35'),\n",
       "  ('au10', 'au77', 'au44'),\n",
       "  ('au10', 'au77', 'au64'),\n",
       "  ('au10', 'au79', 'au84'),\n",
       "  ('au10', 'au81', 'au17'),\n",
       "  ('au10', 'au83', 'au20'),\n",
       "  ('au10', 'au91', 'au63'),\n",
       "  ('au10', 'au95', 'au20'),\n",
       "  ('au10', 'au97', 'au39'),\n",
       "  ('au100', 'au1', 'au60'),\n",
       "  ('au100', 'au1', 'au70'),\n",
       "  ('au100', 'au1', 'au76'),\n",
       "  ('au100', 'au12', 'au38'),\n",
       "  ('au100', 'au12', 'au70'),\n",
       "  ('au100', 'au15', 'au56'),\n",
       "  ('au100', 'au15', 'au66'),\n",
       "  ('au100', 'au25', 'au24'),\n",
       "  ('au100', 'au26', 'au31'),\n",
       "  ('au100', 'au31', 'au10'),\n",
       "  ('au100', 'au35', 'au83'),\n",
       "  ('au100', 'au41', 'au78'),\n",
       "  ('au100', 'au42', 'au84'),\n",
       "  ('au100', 'au45', 'au21'),\n",
       "  ('au100', 'au51', 'au55'),\n",
       "  ('au100', 'au55', 'au80'),\n",
       "  ('au100', 'au56', 'au82'),\n",
       "  ('au100', 'au60', 'au84'),\n",
       "  ('au100', 'au63', 'au21'),\n",
       "  ('au100', 'au63', 'au28'),\n",
       "  ('au100', 'au63', 'au79'),\n",
       "  ('au100', 'au65', 'au28'),\n",
       "  ('au100', 'au66', 'au75'),\n",
       "  ('au100', 'au69', 'au93'),\n",
       "  ('au100', 'au74', 'au67'),\n",
       "  ('au100', 'au74', 'au75'),\n",
       "  ('au100', 'au74', 'au88'),\n",
       "  ('au100', 'au75', 'au77'),\n",
       "  ('au100', 'au76', 'au59')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbest = 100\n",
    "vocab = set()\n",
    "for ngrams in topic_ranked_ngrams.values():\n",
    "    vocab = vocab.union([ngram for ngram, score in ngrams[:nbest]])\n",
    "vocab = sorted(list(vocab))\n",
    "len(vocab), vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Using the selected ngrams, we represent each document as a bag-of-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.zeros((len(train_documents), len(vocab)), dtype=float)\n",
    "train_y = np.zeros((len(train_documents)))\n",
    "for i, docid in enumerate(train_documents):\n",
    "    counter = NGramCounter(order=ngram_order)\n",
    "    counter.add(rawdata[docid])\n",
    "    train_X[i] = counter.get_counts(vocab)\n",
    "    train_y[i] = topics.index(document2topic[docid])\n",
    "    \n",
    "test_X = np.zeros((len(test_documents), len(vocab)), dtype=float)\n",
    "test_y = np.zeros((len(test_documents)))\n",
    "for i, docid in enumerate(test_documents):\n",
    "    counter = NGramCounter(order=ngram_order)\n",
    "    counter.add(rawdata[docid])\n",
    "    test_X[i] = counter.get_counts(vocab)\n",
    "    test_y[i] = topics.index(document2topic[docid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 97., 132., 119., 124., 107.,  78.,  90., 107., 108., 113.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.sum(), test_X.sum(), len(train_X), len(test_X)\n",
    "train_X[:10].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic classification\n",
    "\n",
    "Using \"sklearn\" to build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 18.513 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=13, max_iter=20, tol=1e-1)),\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(clf, parameters, cv=5, iid=False)\n",
    "gs_clf.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "predicted = gs_clf.predict(test_X)\n",
    "print(f'Accuracy: {np.mean(predicted == test_y) * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy for the AUD HMM without features selection: 37.755 % <br>\n",
    "accuracy for the AUD subspace HMM (dim 40): 48.834 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ['\\n'.join(rawdata[doc]) for doc in train_documents]\n",
    "test_corpus = ['\\n'.join(rawdata[doc]) for doc in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(3, 3))\n",
    "train_X = count_vect.fit_transform(train_corpus)\n",
    "test_X = count_vect.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 16.108 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(train_X)\n",
    "train_X_tfidf = tf_transformer.transform(train_X)\n",
    "test_X_tfidf = tf_transformer.transform(test_X)\n",
    "\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=13,\n",
    "                    max_iter=20, tol=1e-3).fit(train_X_tfidf, train_y)\n",
    "\n",
    "predicted = clf.predict(train_X_tfidf)\n",
    "np.mean(predicted == train_y)\n",
    "\n",
    "predicted = clf.predict(test_X_tfidf)\n",
    "print(f'Accuracy: {np.mean(predicted == test_y) * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
