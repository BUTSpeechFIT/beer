{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification \n",
    "\n",
    "This notebook builds a Topic Classification pipeline using BEER. It is mainly thought to work from the output of a AUD model but can be easily adapted to other inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "import pickle \n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import beer\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook, gridplot\n",
    "output_notebook()\n",
    "\n",
    "from utils.ngramfeatures import NGramCounter, select_ngrams\n",
    "import utils.plotting as plotting\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Global configuration of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory structure\n",
    "datadir = 'data'\n",
    "expdir = 'exp'\n",
    "\n",
    "# Input\n",
    "dbname = 'fisher'  \n",
    "model = 'aud_mfcc_8k_4g_gamma_dirichlet_process'\n",
    "\n",
    "# N-gram order for the document representation.\n",
    "ngram_order = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "The following pipeline assume the following directory structure:\n",
    "\n",
    "```\n",
    "<datadir>\n",
    "└── <dbname>\n",
    "    ├── test\n",
    "    │   ├── docids      # list of document ids (test set)\n",
    "    │   └── doclabels   # list of pairs document id <-> topic id (test set)\n",
    "    ├── topics          # list of all the topic ids (optional)\n",
    "    └── train\n",
    "        ├── docids      # list of document ids (train set)\n",
    "        └── doclabels   # list of pairs document id <-> topic id (train set)\n",
    "```\n",
    "\n",
    "The scripts `local/<dbname>/prepare_data.sh <datadir>` will prepare everything except for the documents (`docs`) which you will have to provide to run the recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FISHER data already prepared.\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$datadir\" \"$dbname\"\n",
    "\n",
    "datadir=$1\n",
    "dbname=$2\n",
    "local/$dbname/prepare_data.sh $datadir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the input to the classification. We use the transcription from a AUD system organize in the following way:\n",
    "\n",
    "```\n",
    "<expdir>/<dbname>/<modeldir>\n",
    "├── test\n",
    "│   └── trans\n",
    "└── train\n",
    "    └── trans\n",
    "```\n",
    "\n",
    "The documents are stored in the following way\n",
    "```\n",
    "docid1 first line of the document \n",
    "docid2 first line of the second document\n",
    "docid2 second line of the second document\n",
    "docid1 second line of the first document\n",
    "... \n",
    "```\n",
    "\n",
    "We use the time-aligned transcription of a AUD system converted to a \"standard\" transcription with the tools `recipes/aud/utils/ali2trans.py`. Replace the path to accordingly to run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents for training: 1374\n",
      "number of documents for testing: 1372\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$expdir\" \"$dbname\" \"$model\"\n",
    "\n",
    "expdir=$1\n",
    "dbname=$2\n",
    "model=$3\n",
    "\n",
    "mkdir -p $expdir/$dbname/$model/{test,train}\n",
    "\n",
    "# your code here\n",
    "\n",
    "ntrain=$(cat $expdir/$dbname/$model/train/trans | awk '{print $1}' | sort | uniq | wc -l) \n",
    "ntest=$(cat $expdir/$dbname/$model/test/trans | awk '{print $1}' | sort | uniq | wc -l)\n",
    "echo \"number of documents for training: $ntrain\"\n",
    "echo \"number of documents for testing: $ntest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready. The next cell provides a quick access to the data:\n",
    "  * `topics` list of all the topics sorted alphabetically\n",
    "  * `train_doc_topic` mapping document id -> topic label (train set)\n",
    "  * `test_doc_topic` mapping document id -> topic label (test set)\n",
    "  * `train_docs` mapping document id -> list of sentences (train set)\n",
    "  * `test_docs` mapping document id -> list of sentences (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(datadir, dbname, 'topics'), 'r') as f:\n",
    "    topics = sorted([line.strip() for line in f])\n",
    "\n",
    "def iterate_doclabels(dataset, datadir=datadir, dbname=dbname):\n",
    "    with open(os.path.join(datadir, dbname, dataset, 'doclabels'), 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            yield tokens[0], ' '.join(tokens[1:])\n",
    "train_doc_topic = {docid: topicid for docid, topicid in iterate_doclabels('train')}\n",
    "test_doc_topic = {docid: topicid for docid, topicid in iterate_doclabels('test')}\n",
    "\n",
    "    \n",
    "def iterate_trans(dataset, expdir=expdir, dbname=dbname, model=model):\n",
    "    with open(os.path.join(expdir, dbname, model, dataset, 'trans'), 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            yield tokens[0], ' '.join(tokens[1:])\n",
    "train_docs = defaultdict(list)\n",
    "test_docs = defaultdict(list)\n",
    "for docid, sentence in iterate_trans('train'): train_docs[docid].append(sentence)\n",
    "for docid, sentence in iterate_trans('test'): test_docs[docid].append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that everything is properly setup, we plot the topic distribution for the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"348a00f6-6a9e-472c-ace3-1692f4f087a9\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"15a8a5ee-fa6f-4515-b96e-33eea011ceb1\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1104\",\"type\":\"ToolbarBox\"},{\"id\":\"1102\",\"type\":\"Column\"}]},\"id\":\"1105\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"1085\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{\"grid_line_color\":{\"value\":null},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1014\",\"type\":\"CategoricalTicker\"}},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1038\",\"type\":\"VBar\"},{\"attributes\":{\"formatter\":{\"id\":\"1093\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":1.0471975511965976,\"plot\":{\"id\":\"1043\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1054\",\"type\":\"CategoricalTicker\"}},\"id\":\"1053\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"source\":{\"id\":\"1077\",\"type\":\"ColumnDataSource\"}},\"id\":\"1081\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1039\",\"type\":\"VBar\"},{\"attributes\":{\"plot\":null,\"text\":\"Topic Distribution (train set)\"},\"id\":\"1002\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1054\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"children\":[{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"1091\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1018\",\"type\":\"BasicTicker\"}},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{\"grid_line_color\":{\"value\":null},\"plot\":{\"id\":\"1043\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1054\",\"type\":\"CategoricalTicker\"}},\"id\":\"1056\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1085\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":[0.013100436681222707,0.04294032023289665,0.025473071324599708,0.016739446870451237,0.03056768558951965,0.04512372634643377,0.011644832605531296,0.020378457059679767,0.01455604075691412,0.004366812227074236,0.03711790393013101,0.024745269286754003,0.004366812227074236,0.017467248908296942,0.036390101892285295,0.013100436681222707,0.036390101892285295,0.010917030567685589,0.020378457059679767,0.026928675400291122,0.017467248908296942,0.050946142649199416,0.03711790393013101,0.048034934497816595,0.005094614264919942,0.015283842794759825,0.024017467248908297,0.00727802037845706,0.01455604075691412,0.021834061135371178,0.049490538573508006,0.02911208151382824,0.06331877729257641,0.010917030567685589,0.042212518195050945,0.03784570596797671,0.008005822416302766,0.013828238719068414,0.029839883551673944,0.021106259097525473],\"x\":[\"Affirmative Action\",\"Airport Security\",\"Anonymous Benefactor\",\"Bioterrorism\",\"Censorship\",\"Comedy\",\"Computer Games\",\"Corporate Conduct\",\"Current Events\",\"Drug Testing\",\"Education\",\"Family\",\"Family Values\",\"Food\",\"Foreign Relations\",\"Friends\",\"Health and Fitness\",\"Hobbies\",\"Holidays\",\"Illness\",\"Iraq Arms Inspections\",\"Life Partners\",\"Middle East Issues\",\"Minimum Wage\",\"Money to Leave US\",\"Movies\",\"Opening Own Business\",\"Outdoor Activities\",\"Perjury\",\"Personal Habits\",\"Pets\",\"Reality TV\",\"September 11\",\"Smoking\",\"Sports on TV\",\"Strikes by Athletes\",\"Televised Criminal Trials\",\"Terrorism\",\"Time Travel\",\"US Public Schools\"]},\"selected\":{\"id\":\"1088\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1087\",\"type\":\"UnionRenderers\"}},\"id\":\"1037\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"formatter\":{\"id\":\"1095\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1043\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1058\",\"type\":\"BasicTicker\"}},\"id\":\"1057\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"data\":{\"top\":[0.010932944606413994,0.04664723032069971,0.028425655976676383,0.021137026239067054,0.009475218658892129,0.06268221574344024,0.00510204081632653,0.024052478134110787,0.01239067055393586,0.004373177842565598,0.030612244897959183,0.04227405247813411,0.0036443148688046646,0.018221574344023325,0.021137026239067054,0.01239067055393586,0.016034985422740525,0.01020408163265306,0.029154518950437316,0.03134110787172012,0.02259475218658892,0.04081632653061224,0.018950437317784258,0.033527696793002916,0.016763848396501458,0.015306122448979591,0.0663265306122449,0.013848396501457727,0.028425655976676383,0.02478134110787172,0.030612244897959183,0.033527696793002916,0.038629737609329445,0.014577259475218658,0.02988338192419825,0.016763848396501458,0.006559766763848397,0.028425655976676383,0.052478134110787174,0.02696793002915452],\"x\":[\"Affirmative Action\",\"Airport Security\",\"Anonymous Benefactor\",\"Bioterrorism\",\"Censorship\",\"Comedy\",\"Computer Games\",\"Corporate Conduct\",\"Current Events\",\"Drug Testing\",\"Education\",\"Family\",\"Family Values\",\"Food\",\"Foreign Relations\",\"Friends\",\"Health and Fitness\",\"Hobbies\",\"Holidays\",\"Illness\",\"Iraq Arms Inspections\",\"Life Partners\",\"Middle East Issues\",\"Minimum Wage\",\"Money to Leave US\",\"Movies\",\"Opening Own Business\",\"Outdoor Activities\",\"Perjury\",\"Personal Habits\",\"Pets\",\"Reality TV\",\"September 11\",\"Smoking\",\"Sports on TV\",\"Strikes by Athletes\",\"Televised Criminal Trials\",\"Terrorism\",\"Time Travel\",\"US Public Schools\"]},\"selected\":{\"id\":\"1098\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1097\",\"type\":\"UnionRenderers\"}},\"id\":\"1077\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1087\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"plot\":null,\"text\":\"Topic Distribution (test set)\"},\"id\":\"1042\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1058\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1079\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1088\",\"type\":\"Selection\"},{\"attributes\":{\"callback\":null,\"factors\":[\"Affirmative Action\",\"Airport Security\",\"Anonymous Benefactor\",\"Bioterrorism\",\"Censorship\",\"Comedy\",\"Computer Games\",\"Corporate Conduct\",\"Current Events\",\"Drug Testing\",\"Education\",\"Family\",\"Family Values\",\"Food\",\"Foreign Relations\",\"Friends\",\"Health and Fitness\",\"Hobbies\",\"Holidays\",\"Illness\",\"Iraq Arms Inspections\",\"Life Partners\",\"Middle East Issues\",\"Minimum Wage\",\"Money to Leave US\",\"Movies\",\"Opening Own Business\",\"Outdoor Activities\",\"Perjury\",\"Personal Habits\",\"Pets\",\"Reality TV\",\"September 11\",\"Smoking\",\"Sports on TV\",\"Strikes by Athletes\",\"Televised Criminal Trials\",\"Terrorism\",\"Time Travel\",\"US Public Schools\"]},\"id\":\"1005\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"PanTool\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1043\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1058\",\"type\":\"BasicTicker\"}},\"id\":\"1061\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"children\":[{\"id\":\"1043\",\"subtype\":\"Figure\",\"type\":\"Plot\"}]},\"id\":\"1101\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"below\":[{\"id\":\"1013\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"1017\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1013\",\"type\":\"CategoricalAxis\"},{\"id\":\"1016\",\"type\":\"Grid\"},{\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"id\":\"1021\",\"type\":\"Grid\"},{\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"id\":\"1040\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1002\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1028\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"1005\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"1009\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"1007\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1011\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"below\":[{\"id\":\"1053\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"1057\",\"type\":\"LinearAxis\"}],\"plot_height\":400,\"plot_width\":800,\"renderers\":[{\"id\":\"1053\",\"type\":\"CategoricalAxis\"},{\"id\":\"1056\",\"type\":\"Grid\"},{\"id\":\"1057\",\"type\":\"LinearAxis\"},{\"id\":\"1061\",\"type\":\"Grid\"},{\"id\":\"1070\",\"type\":\"BoxAnnotation\"},{\"id\":\"1080\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1042\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1068\",\"type\":\"Toolbar\"},\"toolbar_location\":null,\"x_range\":{\"id\":\"1045\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"1049\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"1047\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1051\",\"type\":\"LinearScale\"}},\"id\":\"1043\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1093\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.8},\"x\":{\"field\":\"x\"}},\"id\":\"1078\",\"type\":\"VBar\"},{\"attributes\":{\"overlay\":{\"id\":\"1030\",\"type\":\"BoxAnnotation\"}},\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"children\":[{\"id\":\"1091\",\"type\":\"Row\"},{\"id\":\"1101\",\"type\":\"Row\"}]},\"id\":\"1102\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1095\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"SaveTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1077\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1078\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1079\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1081\",\"type\":\"CDSView\"}},\"id\":\"1080\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1098\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1062\",\"type\":\"PanTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1083\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":1.0471975511965976,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1014\",\"type\":\"CategoricalTicker\"}},\"id\":\"1013\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"1063\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"tools\":[{\"id\":\"1022\",\"type\":\"PanTool\"},{\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"id\":\"1025\",\"type\":\"SaveTool\"},{\"id\":\"1026\",\"type\":\"ResetTool\"},{\"id\":\"1027\",\"type\":\"HelpTool\"},{\"id\":\"1062\",\"type\":\"PanTool\"},{\"id\":\"1063\",\"type\":\"WheelZoomTool\"},{\"id\":\"1064\",\"type\":\"BoxZoomTool\"},{\"id\":\"1065\",\"type\":\"SaveTool\"},{\"id\":\"1066\",\"type\":\"ResetTool\"},{\"id\":\"1067\",\"type\":\"HelpTool\"}]},\"id\":\"1103\",\"type\":\"ProxyToolbar\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"ResetTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1070\",\"type\":\"BoxAnnotation\"}},\"id\":\"1064\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null,\"factors\":[\"Affirmative Action\",\"Airport Security\",\"Anonymous Benefactor\",\"Bioterrorism\",\"Censorship\",\"Comedy\",\"Computer Games\",\"Corporate Conduct\",\"Current Events\",\"Drug Testing\",\"Education\",\"Family\",\"Family Values\",\"Food\",\"Foreign Relations\",\"Friends\",\"Health and Fitness\",\"Hobbies\",\"Holidays\",\"Illness\",\"Iraq Arms Inspections\",\"Life Partners\",\"Middle East Issues\",\"Minimum Wage\",\"Money to Leave US\",\"Movies\",\"Opening Own Business\",\"Outdoor Activities\",\"Perjury\",\"Personal Habits\",\"Pets\",\"Reality TV\",\"September 11\",\"Smoking\",\"Sports on TV\",\"Strikes by Athletes\",\"Televised Criminal Trials\",\"Terrorism\",\"Time Travel\",\"US Public Schools\"]},\"id\":\"1045\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"1065\",\"type\":\"SaveTool\"},{\"attributes\":{\"data_source\":{\"id\":\"1037\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1038\",\"type\":\"VBar\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1039\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"1041\",\"type\":\"CDSView\"}},\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1022\",\"type\":\"PanTool\"},{\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"id\":\"1025\",\"type\":\"SaveTool\"},{\"id\":\"1026\",\"type\":\"ResetTool\"},{\"id\":\"1027\",\"type\":\"HelpTool\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"ResetTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"HelpTool\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1062\",\"type\":\"PanTool\"},{\"id\":\"1063\",\"type\":\"WheelZoomTool\"},{\"id\":\"1064\",\"type\":\"BoxZoomTool\"},{\"id\":\"1065\",\"type\":\"SaveTool\"},{\"id\":\"1066\",\"type\":\"ResetTool\"},{\"id\":\"1067\",\"type\":\"HelpTool\"}]},\"id\":\"1068\",\"type\":\"Toolbar\"},{\"attributes\":{\"source\":{\"id\":\"1037\",\"type\":\"ColumnDataSource\"}},\"id\":\"1041\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null},\"id\":\"1047\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1097\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1070\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"toolbar\":{\"id\":\"1103\",\"type\":\"ProxyToolbar\"},\"toolbar_location\":\"above\"},\"id\":\"1104\",\"type\":\"ToolbarBox\"}],\"root_ids\":[\"1105\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.2\"}};\n",
       "  var render_items = [{\"docid\":\"15a8a5ee-fa6f-4515-b96e-33eea011ceb1\",\"roots\":{\"1105\":\"348a00f6-6a9e-472c-ace3-1692f4f087a9\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1105"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_topic_counts = defaultdict(int)\n",
    "for docid, topicid in train_doc_topic.items():\n",
    "    train_topic_counts[topicid] += 1\n",
    "tot = sum(train_topic_counts.values())\n",
    "train_topic_prob = [train_topic_counts[topic] / tot for topic in topics]\n",
    "    \n",
    "test_topic_counts = defaultdict(int)\n",
    "for docid, topicid in test_doc_topic.items():\n",
    "    test_topic_counts[topicid] += 1\n",
    "tot = sum(test_topic_counts.values())\n",
    "test_topic_prob = [test_topic_counts[topic] / tot for topic in topics]\n",
    "\n",
    "fig1 = figure(title='Topic Distribution (train set)', x_range=topics, \n",
    "              width=800, height=400)\n",
    "fig1.vbar(x=topics, top=train_topic_prob, width=0.8)\n",
    "fig1.xaxis.major_label_orientation = math.pi/3\n",
    "fig1.xgrid.grid_line_color = None\n",
    "\n",
    "fig2 = figure(title='Topic Distribution (test set)', x_range=topics, \n",
    "              width=800, height=400)\n",
    "fig2.vbar(x=topics, top=test_topic_prob, width=0.8)\n",
    "fig2.xaxis.major_label_orientation = math.pi/3\n",
    "fig2.xgrid.grid_line_color = None\n",
    "\n",
    "show(gridplot([[fig1], [fig2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction\n",
    "\n",
    "Each document is represented by a 'bag-of-ngrams'. Because the vocabulary (i.e. the number of unique n-gram) can be quite large, we select the subset of n-grams which have the highest joint conditional probability:\n",
    "\n",
    "$$\n",
    "p(t | w ) = \\frac{f_{wt} + |T|p(t)}{f_w + |T|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "  * $t$ is a topic label\n",
    "  * $w$ is a n-gram\n",
    "  * $|T|$ is the total number of topics\n",
    "  * $f_wt$ is the number of times the n-gram $w$ occurs in all documents associated to the topic $t$\n",
    "  * $f_w$ is the number of times the n-gram $w$ occurs in the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3975\n",
      "A few samples of the selected n-grams: \n",
      "('au66', 'au80', 'au94')\n",
      "('au73', 'au51', 'au89')\n",
      "('au99', 'au61', 'au20')\n",
      "('au84', 'au40', 'au33')\n",
      "('au6', 'au11', 'au31')\n",
      "('au35', 'au77', 'au54')\n",
      "('au13', 'au91', 'au78')\n",
      "('au50', 'au60', 'au18')\n",
      "('au94', 'au6', 'au39')\n",
      "('au50', 'au72', 'au5')\n"
     ]
    }
   ],
   "source": [
    "vocab = select_ngrams(\n",
    "    ngram_order, \n",
    "    nbest=100, \n",
    "    corpus=iterate_trans('train'), \n",
    "    doc_topic=train_doc_topic, \n",
    "    topic_prob={topicid: prob for topicid, prob in zip(topics, train_topic_prob)}\n",
    ")\n",
    "\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "print('A few samples of the selected n-grams: ')\n",
    "for ngram in random.sample(vocab, min(10, len(vocab))):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the corpus into a matrix as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{pmatrix}\n",
    "    f_{11} & f_{12} & \\dots & f_{1D} \\\\\n",
    "    \\vdots & \\ddots & & \\vdots \\\\\n",
    "    f_{M1} & f_{M2} & \\dots & f_{MD}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where:\n",
    "  * $D$ is the number of documents in the corpus\n",
    "  * $M$ is the size the vocabulary, i.e. the number of selected n-gram\n",
    "  \n",
    "Similarly, we encode the document labels into a vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{pmatrix} t_1, t_d, \\dots, t_D \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $t_i$ is the (index of) topic label of the $i$th document\n",
    "\n",
    "NOTE: our implementation is not very memory efficient as we store a dense matrix even though the data is very sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: torch.Size([1374, 3975])\n",
      "test set size: torch.Size([1372, 3975])\n"
     ]
    }
   ],
   "source": [
    "train_X = np.zeros((len(train_docs), len(vocab)), dtype=float)\n",
    "train_y = np.zeros((len(train_docs)))\n",
    "for i, docid in enumerate(train_docs):\n",
    "    counter = NGramCounter(ngram_order)\n",
    "    for sentence in train_docs[docid]: counter.add(sentence)\n",
    "    train_X[i] = counter.get_counts(vocab)\n",
    "    train_y[i] = topics.index(train_doc_topic[docid])\n",
    "    \n",
    "test_X = np.zeros((len(test_docs), len(vocab)), dtype=float)\n",
    "test_y = np.zeros((len(test_docs)))\n",
    "for i, docid in enumerate(test_docs):\n",
    "    counter = NGramCounter(ngram_order)\n",
    "    for sentence in test_docs[docid]: counter.add(sentence)\n",
    "    test_X[i] = counter.get_counts(vocab)\n",
    "    test_y[i] = topics.index(test_doc_topic[docid])\n",
    "    \n",
    "train_X = torch.from_numpy(train_X)\n",
    "train_y = torch.from_numpy(train_y).long()\n",
    "test_X = torch.from_numpy(test_X)\n",
    "test_y = torch.from_numpy(test_y).long()\n",
    "    \n",
    "print(f'train set size: {train_X.shape}')\n",
    "print(f'test set size: {test_X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model\n",
    "\n",
    "We use a revisited version of the Subspace Mutlinomial Model (SMM) to model the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 50\n",
    "\n",
    "# Data model (SGMM).\n",
    "modelset = beer.NormalSet.create(\n",
    "    mean=torch.zeros(latent_dim), \n",
    "    cov=torch.ones(latent_dim),\n",
    "    size=len(topics),\n",
    "    cov_type='full'\n",
    ")\n",
    "latent_prior = beer.Mixture.create(modelset).double()\n",
    "\n",
    "# Build the Multinomial model.\n",
    "mean = torch.ones(len(vocab)) / len(vocab)\n",
    "model = beer.Categorical.create(mean).double()\n",
    "newparams = {\n",
    "    param: beer.SubspaceBayesianParameter.from_parameter(param, latent_prior)\n",
    "    for param in model.bayesian_parameters()\n",
    "}\n",
    "model.replace_parameters(newparams)\n",
    "\n",
    "# Create the Generalized Subspace Model\n",
    "gsm = beer.GSM.create(model, latent_dim, latent_prior).double()\n",
    "\n",
    "# Create the instance of SGMM for each dataset\n",
    "models, latent_posts = gsm.new_models(len(train_docs), cov_type='diagonal')\n",
    "\n",
    "# Accumulate the statistics.\n",
    "for model in models:\n",
    "    dim = train_X.shape[1]\n",
    "    data = torch.eye(dim, dtype=train_X.dtype, device=train_X.device)\n",
    "    data[range(dim), range(dim)] = train_X[i]\n",
    "    elbo = beer.evidence_lower_bound(model, train_X)\n",
    "    elbo.backward(std_params=False)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# Optimizer for the conjugate parameters.\n",
    "params = gsm.conjugate_bayesian_parameters(keepgroups=True)\n",
    "cjg_optim = beer.VBConjugateOptimizer(params, lrate=1e-2)\n",
    "\n",
    "# Optimizer for the standard parameters.\n",
    "params = list(latent_posts.parameters()) + list(gsm.parameters())\n",
    "std_optim = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "# Global optimizer.\n",
    "optim = beer.VBOptimizer(cjg_optim, std_optim)\n",
    "\n",
    "elbos = []\n",
    "for epoch in range(1, epochs + 1): \n",
    "    for batch in range(0, len(train_docs), batch_size):\n",
    "        batch_idxs = random.sample(range(len(models)), k=batch_size)\n",
    "        batch_models = [models[s] for s in batch_idxs]\n",
    "        batch_latent_posts = latent_posts[batch_idxs]\n",
    "        batch_labels = train_y[batch_idxs]\n",
    "        \n",
    "        optim.init_step()\n",
    "        elbo = beer.evidence_lower_bound(\n",
    "            gsm, \n",
    "            batch_models, \n",
    "            labels=batch_labels,\n",
    "            latent_posts=batch_latent_posts, \n",
    "            latent_nsamples=1, \n",
    "            params_nsamples=1,\n",
    "            datasize=len(models)\n",
    "        )\n",
    "        elbo.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        elbos.append(float(elbo))\n",
    "\n",
    "fig = figure()\n",
    "fig.line([epochs * e/len(elbos) for e in range(len(elbos))],\n",
    "         elbos)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latent_posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b7d35f34a563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_posts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_prior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy: {100 * np.mean(pred == train_y):.5f} %'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_posts' is not defined"
     ]
    }
   ],
   "source": [
    "means = latent_posts.params.mean\n",
    "pred = latent_prior.posteriors(means).argmax(dim=1).detach().numpy()\n",
    "\n",
    "print(f'Accuracy: {100 * np.mean(pred == train_y):.5f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0505,  0.0697, -0.0406,  0.0295, -0.0198,  0.0505, -0.0629, -0.0046,\n",
       "        -0.0217, -0.0671,  0.0528,  0.0230,  0.0701,  0.0235,  0.0232,  0.0121,\n",
       "        -0.0742, -0.0159,  0.0282, -0.0015,  0.0308,  0.0507, -0.0261, -0.0719,\n",
       "         0.0316, -0.0259,  0.0643,  0.0394, -0.0676,  0.0250,  0.0566, -0.0631,\n",
       "         0.0198, -0.0827,  0.0052,  0.0072, -0.0434, -0.0554, -0.0698, -0.0721,\n",
       "         0.0618,  0.0156,  0.0272,  0.0269,  0.0520, -0.0181,  0.0312, -0.0019,\n",
       "         0.0174, -0.0549], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_prior.modelset[0].mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (50,2) and (50,50) not aligned: 2 (dim 1) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-86b07ad5c37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/beer/recipes/tid/utils/plotting.py\u001b[0m in \u001b[0;36mplot_gmm\u001b[0;34m(fig, gmm, n_std_dev, npoints, alpha, colors, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         plot_normal(fig, comp.mean.detach().numpy(), comp.cov.detach().numpy(),\n\u001b[0;32m---> 49\u001b[0;31m             n_std_dev, npoints, alpha=alpha * weight.detach().numpy(), **kwargs)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_hmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_std_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/beer/recipes/tid/utils/plotting.py\u001b[0m in \u001b[0;36mplot_normal\u001b[0;34m(fig, mean, cov, n_std_dev, npoints, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     plot_covariance(fig, cov, n_std_dev, npoints, transform=transform,\n\u001b[0;32m---> 40\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/beer/recipes/tid/utils/plotting.py\u001b[0m in \u001b[0;36mplot_covariance\u001b[0;34m(fig, covariance, n_std_dev, npoints, transform, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstd_dev\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_std_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         plot_circle(fig, std_dev, npoints, tensor_metric=tensor_metric.T, \n\u001b[0;32m---> 32\u001b[0;31m                     transform=transform, **kwargs)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_std_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/beer/recipes/tid/utils/plotting.py\u001b[0m in \u001b[0;36mplot_circle\u001b[0;34m(fig, radius, npoints, tensor_metric, transform, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m def plot_circle(fig, radius, npoints, tensor_metric=np.eye(2), transform=None, \n\u001b[1;32m     19\u001b[0m                 **kwargs):\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mxy1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_upper_semicircle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpoints\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtensor_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mxy2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lower_semicircle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpoints\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtensor_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (50,2) and (50,50) not aligned: 2 (dim 1) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "fig = figure()\n",
    "\n",
    "plotting.plot_gmm(fig, latent_prior, alpha=.5)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTOPICS = 40  # Number of topics {6, 40}\n",
    "#datapath = 'data/aud_subspace_mbn_4g_gamma_dirichlet_process_ldim40.trans'\n",
    "#datapath = 'data/aud_subspace_mbn_4g_gamma_dirichlet_process_ldim100.trans'\n",
    "#datapath = 'data/aud_mbn_4g_gamma_dirichlet_process.trans'\n",
    "datapath = 'data/aud_mfcc_8k_4g_gamma_dirichlet_process.trans'\n",
    "\n",
    "\n",
    "with open(f'data/fisher_{NTOPICS}c_train.flist', 'r') as f:\n",
    "    train_docs = [line.strip() for line in f]\n",
    "    \n",
    "with open(f'data/fisher_{NTOPICS}c_test.flist', 'r') as f:\n",
    "    test_docs = [line.strip() for line in f]\n",
    "    \n",
    "with open('data/tID_tName.pkl', 'rb') as f:\n",
    "    topic_names = pickle.load(f)\n",
    "        \n",
    "# Load all documents with their associated topic label.\n",
    "#document2topic = {}                                                            \n",
    "#with open('data/fe_03_p1_calldata.tbl', 'r') as f:                                            \n",
    "#    next(f) # skip the first line.                                             \n",
    "#    for line in f:                                                             \n",
    "#        tokens = line.strip().split(',')                                       \n",
    "#        docid, topicid = tokens[0], tokens[2]             \n",
    "#        if topicid != '' and (docid in train_docs or docid in test_docs):                              \n",
    "#            document2topic[docid] = topicid     \n",
    "            \n",
    "document2topic = {}\n",
    "with open('data/fID_tID.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "for doc, topicid in labels.items():\n",
    "    document2topic[doc] = topic_names[topicid]\n",
    "    \n",
    "        \n",
    "train_documents = []\n",
    "test_documents = []\n",
    "for docid in document2topic:\n",
    "    if docid in train_docs:\n",
    "        train_documents.append(docid)\n",
    "    elif docid in test_docs:\n",
    "        test_documents.append(docid)\n",
    "            \n",
    "# Build the reverse mapping topic -> documents\n",
    "topic2document = defaultdict(list)\n",
    "for doc, topic in document2topic.items():\n",
    "    topic2document[topic].append(doc)\n",
    "topics = sorted(list(topic2document.keys()))\n",
    "\n",
    "topic2document_train = defaultdict(list)\n",
    "for doc, topic in document2topic.items():\n",
    "    if doc in train_docs:\n",
    "        topic2document_train[topic].append(doc)\n",
    "\n",
    "topic2document_test = defaultdict(list)\n",
    "for doc, topic in document2topic.items():\n",
    "    if doc in test_docs:\n",
    "        topic2document_test[topic].append(doc)\n",
    "\n",
    "# Load the raw data\n",
    "rawdata = defaultdict(list)                                                     \n",
    "with open(datapath, 'r') as f:                                                  \n",
    "    for line in f:                                                          \n",
    "        tokens = line.strip().split()                                       \n",
    "        docid = tokens[0].replace('fe_03_', '')[:5]                        \n",
    "        rawdata[docid].append(' '.join(tokens[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features selection\n",
    "\n",
    "The vocabulary is selected per topic by choosing the $K$ n-gram $w$ with the highest following propbability:\n",
    "\n",
    "$$\n",
    "p(t | w ) = \\frac{f_{wt} + |T|p(t)}{f_w + |T|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_t = np.array([float(len(topic2document_train[topic])) for topic in topics]) \n",
    "p_t /= p_t.sum()\n",
    "ngram_order = 3\n",
    "\n",
    "class NGramCounter:\n",
    "    \n",
    "    def __init__(self, order=3, prior_count=0.):\n",
    "        self.order = order\n",
    "        self.counts = defaultdict(lambda: prior_count)\n",
    "        \n",
    "    def add(self, doc):\n",
    "        for utt in doc:\n",
    "            new_utt = '<s> ' * (self.order - 1)  + utt\n",
    "            tokens = new_utt.split()\n",
    "            for i in range(len(tokens) - self.order):\n",
    "                ngram = tuple(tokens[i:i+self.order])\n",
    "                self.counts[ngram] += 1\n",
    "                \n",
    "    def get_counts(self, vocab=None):\n",
    "        if vocab is None:\n",
    "            vocab = sorted(list(self.counts.keys()))\n",
    "        return [self.counts[word] for word in vocab]\n",
    "        \n",
    "# Evaluate the counts. \n",
    "global_counter = NGramCounter(order=ngram_order, prior_count=NTOPICS)\n",
    "topic_counters = {}\n",
    "for p_topic, topic in zip(p_t, topics):\n",
    "    topic_counters[topic] = NGramCounter(order=ngram_order, prior_count=NTOPICS * p_topic)\n",
    "    for doc in topic2document_train[topic]:\n",
    "        global_counter.add(rawdata[doc])\n",
    "        topic_counters[topic].add(rawdata[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ranked_ngrams = {}\n",
    "for topic in topics:\n",
    "    ranked_ngrams = []\n",
    "    for ngram in global_counter.counts:\n",
    "        score = topic_counters[topic].counts[ngram] / global_counter.counts[ngram]\n",
    "        ranked_ngrams.append((ngram, score))\n",
    "    topic_ranked_ngrams[topic]  = list(reversed(sorted(ranked_ngrams, key=lambda x: x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbest = 100\n",
    "vocab = set()\n",
    "for ngrams in topic_ranked_ngrams.values():\n",
    "    vocab = vocab.union([ngram for ngram, score in ngrams[:nbest]])\n",
    "vocab = sorted(list(vocab))\n",
    "len(vocab), vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Using the selected ngrams, we represent each document as a bag-of-ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.zeros((len(train_documents), len(vocab)), dtype=float)\n",
    "train_y = np.zeros((len(train_documents)))\n",
    "for i, docid in enumerate(train_documents):\n",
    "    counter = NGramCounter(order=ngram_order)\n",
    "    counter.add(rawdata[docid])\n",
    "    train_X[i] = counter.get_counts(vocab)\n",
    "    train_y[i] = topics.index(document2topic[docid])\n",
    "    \n",
    "test_X = np.zeros((len(test_documents), len(vocab)), dtype=float)\n",
    "test_y = np.zeros((len(test_documents)))\n",
    "for i, docid in enumerate(test_documents):\n",
    "    counter = NGramCounter(order=ngram_order)\n",
    "    counter.add(rawdata[docid])\n",
    "    test_X[i] = counter.get_counts(vocab)\n",
    "    test_y[i] = topics.index(document2topic[docid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.sum(), test_X.sum(), len(train_X), len(test_X)\n",
    "train_X[:10].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic classification\n",
    "\n",
    "Using \"sklearn\" to build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 18.586 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=13, max_iter=20, tol=1e-1)),\n",
    "])\n",
    "\n",
    "\n",
    "parameters = {\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(clf, parameters, cv=5, iid=False)\n",
    "gs_clf.fit(train_X.numpy(), train_y.numpy())\n",
    "\n",
    "\n",
    "predicted = gs_clf.predict(test_X.numpy())\n",
    "print(f'Accuracy: {np.mean(predicted == test_y.numpy()) * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy for the AUD HMM without features selection: 37.755 % <br>\n",
    "accuracy for the AUD subspace HMM (dim 40): 48.834 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ['\\n'.join(rawdata[doc]) for doc in train_documents]\n",
    "test_corpus = ['\\n'.join(rawdata[doc]) for doc in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(3, 3))\n",
    "train_X = count_vect.fit_transform(train_corpus)\n",
    "test_X = count_vect.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 16.108 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(train_X)\n",
    "train_X_tfidf = tf_transformer.transform(train_X)\n",
    "test_X_tfidf = tf_transformer.transform(test_X)\n",
    "\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=13,\n",
    "                    max_iter=20, tol=1e-3).fit(train_X_tfidf, train_y)\n",
    "\n",
    "predicted = clf.predict(train_X_tfidf)\n",
    "np.mean(predicted == train_y)\n",
    "\n",
    "predicted = clf.predict(test_X_tfidf)\n",
    "print(f'Accuracy: {np.mean(predicted == test_y) * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
