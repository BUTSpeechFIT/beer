{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Subspace Model: Normal distribution\n",
    "\n",
    "This notebook illustrates how to build and train a Bayesian Generalized Subspace Model for a Normal distribution with the [beer framework](https://github.com/beer-asr/beer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"beer\" to the PYTHONPATH\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import beer\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# For plotting.\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from bokeh.models import Range1d, LinearAxis\n",
    "output_notebook()\n",
    "\n",
    "# Convenience functions for plotting.\n",
    "import plotting\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The \"data\" is actually a Normal distribution whose parameters lie in a (non-linear) subspace. For our synthetic example it is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    x & \\sim \\mathcal{N}(\\frac{3}{2}, 10) \\\\\n",
    "    \\mu &= \n",
    "        \\begin{pmatrix}\n",
    "            x \\\\\n",
    "            1 - \\ln \\big( 1 + e^{-x} \\big)\n",
    "        \\end{pmatrix} \\\\\n",
    "    \\Sigma &= \n",
    "        \\begin{pmatrix}\n",
    "            1 & 0 \\\\\n",
    "            0 & 1 + e^{-\\frac{x}{2}}\n",
    "        \\end{pmatrix}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_from_subspace(x):\n",
    "    mean = np.array([x, 1 - np.log(1 + np.exp(-x))])\n",
    "    cov = np.array([\n",
    "        [1., 0.],\n",
    "        [0., (1 + np.exp(-.5 * x))]\n",
    "    ])\n",
    "    return mean, cov \n",
    "\n",
    "xs = np.sqrt(10) * np.random.randn(100) + 1.5\n",
    "dists = [normal_from_subspace(x) for x in xs]\n",
    "\n",
    "fig = figure(\n",
    "    title='Data',\n",
    "    width=400,\n",
    "    height=400,\n",
    "    x_range=(-10, 10),\n",
    "    y_range=(-10, 10)\n",
    ")\n",
    "\n",
    "for dist in dists:\n",
    "    plotting.plot_normal(fig, dist[0], dist[1], alpha=.1)\n",
    "    \n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural parameters of the Normal distribution.\n",
    "\n",
    "$$\n",
    "\\eta(\\tau, \\mu) = \\begin{pmatrix}\n",
    "    -\\frac{\\tau}{2} \\\\\n",
    "    \\tau \\mu \\\\\n",
    "    - \\frac{\\tau \\mu^2}{2} \\\\\n",
    "    \\frac{1}{2} \\ln \\tau\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def natural_params(mean, diag_prec):\n",
    "    return np.hstack([\n",
    "        -.5 * diag_prec,\n",
    "        diag_prec * mean,\n",
    "        -.5 * diag_prec * (mean ** 2),\n",
    "        .5 * np.log(diag_prec)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack([natural_params(dist[0], 1 / np.diag(dist[1])) for dist in dists])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "We create two types of Normal distribution: one diagonal covariance matrix and another one with full covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimension of the observed spac.\n",
    "obs_dim = data.shape[1]\n",
    "\n",
    "# Dimension of the latent space. It can be bigger or smaller\n",
    "# than the dimension of the observed space.\n",
    "latent_dim = 2\n",
    "\n",
    "# Number of samples for the \"reparameterization-trick\".\n",
    "nb_samples = 10\n",
    "\n",
    "# Number of units per hidden-layer.\n",
    "n_units = 10\n",
    "\n",
    "# beer uses pytorch as a backend for the neural-network part\n",
    "# of the model.\n",
    "from torch import nn\n",
    "\n",
    "# Neural network structure of the encoder of the model.\n",
    "enc_struct = nn.Sequential(\n",
    "    nn.Linear(obs_dim, n_units),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_units, n_units),\n",
    "    nn.Tanh()\n",
    ")\n",
    "encoder = beer.models.MLPNormalDiag(enc_struct, latent_dim, residual=True)\n",
    "\n",
    "# Neural network structure of the decoder of the model.\n",
    "dec_struct = nn.Sequential(\n",
    "    nn.Linear(latent_dim, n_units),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_units, n_units),\n",
    "    nn.Tanh()\n",
    ")\n",
    "decoder = beer.models.MLPNormalGamma(dec_struct, obs_dim, prior_count=1)\n",
    "\n",
    "# Model of the latent space (uncomment the one you want to try).\n",
    "# It can be changed at any-time. \n",
    "# ----------------------------------------------------------------------\n",
    "latent_model = beer.models.NormalDiagonalCovariance.create(latent_dim)\n",
    "\n",
    "#latent_model = beer.models.NormalFullCovariance.create(latent_dim, prior_count=1e-3)\n",
    "\n",
    "#args = {'dim':2, 'prior_count':1, 'mean': data_mean, 'cov': data_cov, 'random_init':True}\n",
    "#latent_model = beer.Mixture.create(10, beer.NormalDiagonalCovariance.create, args, prior_count=1e-6)\n",
    "\n",
    "#args = {'dim':2, 'prior_count':1, 'mean': data_mean, 'cov': data_cov, 'random_init':True}\n",
    "#latent_model = beer.Mixture.create(10, beer.NormalFullCovariance.create, args, prior_count=1)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Putting everything together to build the SVAE.\n",
    "svae = beer.models.VAE(encoder, decoder, latent_model, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beer.training.train_vae(svae, data, max_epochs=500, lrate=1e-3, latent_model_lrate=0, kl_weight=0.0, callback=None)\n",
    "beer.training.train_vae(svae, data, max_epochs=500, lrate=0, latent_model_lrate=1e-1, kl_weight=0.0, callback=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Callback to monitor the training progress.\n",
    "elbos, llhs, klds = [], [], []\n",
    "def callback(elbo, llh, kld):\n",
    "    elbos.append(elbo)\n",
    "    llhs.append(llh)\n",
    "    klds.append(kld)\n",
    "\n",
    "# This is the training.\n",
    "beer.training.train_vae(svae, data, max_epochs=5000, lrate=1e-3, latent_model_lrate=1e-2, callback=callback)\n",
    "\n",
    "# Plot the ELBO.\n",
    "fig1 = figure(title='ELBO', width=400, height=400, x_axis_label='step',\n",
    "              y_axis_label='ln p(X)')\n",
    "fig1.line(np.arange(len(elbos)), elbos)\n",
    "\n",
    "# Plot the LLH and the KLD separately.\n",
    "fig2 = figure(title='LLH + KLD', width=400, height=400,\n",
    "              y_range=(min(llhs) - 1, max(llhs) + 1),\n",
    "              x_axis_label='step', y_axis_label='ln p(x|...)')\n",
    "fig2.line(np.arange(len(llhs)), llhs)\n",
    "fig2.extra_y_ranges['KLD'] = Range1d(0, max(klds) + 1)\n",
    "fig2.add_layout(LinearAxis(y_range_name=\"KLD\", axis_label='KLD'), 'right')\n",
    "fig2.line(np.arange(len(klds)), klds, y_range_name='KLD', color='green')\n",
    "\n",
    "show(gridplot([[fig1, fig2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the model has learned, we sample \"embeddings\" from the prior distribution and transform them with decoder of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed space\n",
    "fig1 = figure(x_range=(-10, 10), y_range=(-10, 10), width=400, height=400)\n",
    "eps = Variable(torch.from_numpy(np.random.multivariate_normal(svae.latent_model.mean, svae.latent_model.cov, 100)).float())\n",
    "decoder_state = svae.decoder(eps)\n",
    "for prior in decoder_state.as_priors():\n",
    "    normal = beer.NormalDiagonalCovariance(prior, prior)\n",
    "    plotting.plot_normal(fig1, normal.mean, normal.cov, alpha=.1)\n",
    "\n",
    "# Latent Space.\n",
    "fig2 = figure(x_range=(-10, 10), y_range=(-10, 10), width=400, height=400)\n",
    "embeddings = eps.data.numpy()\n",
    "plotting.plot_latent_model(fig2, svae.latent_model, color='salmon')\n",
    "fig2.cross(embeddings[:, 0], embeddings[:, 1], color='black')\n",
    "\n",
    "grid = gridplot([[fig1, fig2]])\n",
    "show(grid)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
