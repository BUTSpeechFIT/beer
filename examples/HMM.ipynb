{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Mixture Model\n",
    "\n",
    "This notebook illustrate how to build and train a Bayesian Hidden Markov Model with the [beer framework](https://github.com/beer-asr/beer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"beer\" to the PYTHONPATH\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import copy\n",
    "\n",
    "import beer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For plotting.\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "output_notebook()\n",
    "\n",
    "# Convenience functions for plotting.\n",
    "import plotting\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Generate data following HMM generative process\n",
    "#### Probability of initial states\n",
    "$$\n",
    "p(s^0 = s_1) = 1 \\\\\n",
    "p(s^0 = s_2) = 0 \\\\\n",
    "p(s^0 = s_3) = 0\n",
    "$$\n",
    "\n",
    "#### Probability of transitions\n",
    "$$\n",
    "p(s^t = s_1 \\vert s^{t-1} = s_1) = 0.5 \\quad p(s^t = s_2 \\vert s^{t-1} = s_1) = 0.5 \\quad p(s^t = s_3 \\vert s^{t-1} = s_1) = 0 \\\\\n",
    "p(s^t = s_1 \\vert s^{t-1} = s_2) = 0 \\quad p(s^t = s_2 \\vert s^{t-1} = s_2) = 0.5 \\quad p(s^t = s_3 \\vert s^{t-1} = s_2) = 0.5 \\\\\n",
    "p(s^t = s_1 \\vert s^{t-1} = s_3) = 0.5 \\quad p(s^t = s_2 \\vert s^{t-1} = s_3) = 0 \\quad p(s^t = s_3 \\vert s^{t-1} = s_3) = 0.5 \\\\  \n",
    "$$\n",
    "\n",
    "#### Emission\n",
    "$$\n",
    "p(x^t \\vert s^t = s_1) = \\mathcal{N}(x^t \\vert \\mu_1, \\Sigma_1) \\\\\n",
    "p(x^t \\vert s^t = s_2) = \\mathcal{N}(x^t \\vert \\mu_2, \\Sigma_2) \\\\\n",
    "p(x^t \\vert s^t = s_3) = \\mathcal{N}(x^t \\vert \\mu_3, \\Sigma_3)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "ndim = 2\n",
    "nstates = 3\n",
    "trans_mat = np.array([[.5, .5, 0], [0, .5, .5], [.5, 0, .5]])\n",
    "\n",
    "means = [np.array([-1.5, 4]) * 2,np.array([5, 5]) * 2, np.array([1, -2])] * 2\n",
    "covs = [np.array([[.75, -.5], [-.5, 2.]]), np.array([[2, 1], [1, .75]]), np.array([[1, 0], [0, 1]]) ]\n",
    "normal_sets = [[means[0], covs[0]], [means[1], covs[1]], [means[2], covs[2]]] \n",
    "\n",
    "states = np.zeros(nsamples, dtype=int)\n",
    "data = np.zeros((nsamples, ndim))\n",
    "states[0] = 0\n",
    "data[0] = np.random.multivariate_normal(means[states[0]], covs[states[0]], size=1)\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "fig1 = figure(title='Samples', width=400, height=400)\n",
    "fig1.circle(data[0, 0], data[0, 1], color=colors[states[0]])\n",
    "\n",
    "\n",
    "for n in range(1, nsamples):\n",
    "    states[n] = np.random.choice(np.arange(nstates), p=trans_mat[states[n-1]])\n",
    "    data[n] = np.random.multivariate_normal(means[states[n]], covs[states[n]], size=1)\n",
    "    fig1.circle(data[n, 0], data[n, 1], color=colors[states[n]], line_width=1)\n",
    "    fig1.line(data[n-1:n+1, 0], data[n-1:n+1, 1], color='black', line_width=.5, alpha=.5)\n",
    "\n",
    "fig2 = figure(title='Emissions',  width=400, height=400)\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, n in enumerate(normal_sets):\n",
    "    plotting.plot_normal(fig2, n[0], n[1], alpha=.3, color=colors[i])\n",
    "grid = gridplot([[fig1, fig2]])\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "We create several types of HMMs, each of them has the same transition matrix and initial / final state probability, and a specific type of emission density: \n",
    "  * one Normal density per state with full covariance matrix\n",
    "  * one Normal density per state with diagonal covariance matrix\n",
    "  * one Normal density per state with full covariance matrix shared across states\n",
    "  * one Normal density per state with diagonal covariance matrix shared across states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = beer.graph.Graph()\n",
    "\n",
    "# Initial and final state are non-emitting.\n",
    "s0 = graph.add_state()\n",
    "s4 = graph.add_state()\n",
    "graph.start_state = s0\n",
    "graph.end_state = s4\n",
    "\n",
    "s1 = graph.add_state(pdf_id=0)\n",
    "s2 = graph.add_state(pdf_id=1)\n",
    "s3 = graph.add_state(pdf_id=2)\n",
    "graph.add_arc(s0, s1)\n",
    "graph.add_arc(s1, s1)\n",
    "graph.add_arc(s1, s2)\n",
    "graph.add_arc(s2, s2)\n",
    "graph.add_arc(s2, s3)\n",
    "graph.add_arc(s3, s3)\n",
    "graph.add_arc(s3, s1)\n",
    "graph.add_arc(s1, s4)\n",
    "graph.add_arc(s2, s4)\n",
    "graph.add_arc(s3, s4)\n",
    "\n",
    "graph.normalize()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgraph = graph.compile()\n",
    "cgraph.final_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the global mean/cov. matrix of the data to initialize the mixture.\n",
    "data_mean = torch.from_numpy(data.mean(axis=0)).float()\n",
    "data_var = torch.from_numpy(np.cov(data.T)).float()\n",
    "\n",
    "init_states = torch.LongTensor([0])\n",
    "final_states = torch.LongTensor([2])\n",
    "transitions = torch.from_numpy(trans_mat).float()\n",
    "\n",
    "# HMM (isotropic cov).\n",
    "modelset = beer.NormalSet.create(data_mean, data_var, size=len(transitions),\n",
    "                                prior_strength=1., noise_std=0, \n",
    "                                cov_type='isotropic')\n",
    "hmm_iso = beer.HMM.create(cgraph, modelset)\n",
    "\n",
    "# HMM (diag cov).\n",
    "modelset = beer.NormalSet.create(data_mean, data_var, size=len(transitions),\n",
    "                                prior_strength=1., noise_std=0, \n",
    "                                cov_type='diagonal')\n",
    "hmm_diag = beer.HMM.create(cgraph, modelset)\n",
    "\n",
    "# HMM (full cov).\n",
    "modelset = beer.NormalSet.create(data_mean, data_var, size=len(transitions),\n",
    "                                prior_strength=1., noise_std=0, \n",
    "                                cov_type='full')\n",
    "hmm_full = beer.HMM.create(cgraph, modelset)\n",
    "\n",
    "# HMM (shared isotropic cov).\n",
    "modelset = beer.NormalSet.create(data_mean, data_var, size=len(transitions),\n",
    "                                prior_strength=1., noise_std=0, \n",
    "                                cov_type='isotropic', shared_cov=True)\n",
    "hmm_shared_iso = beer.HMM.create(cgraph, modelset)\n",
    "\n",
    "# HMM (shared diag cov).\n",
    "modelset = beer.NormalSet.create(data_mean, data_var, size=len(transitions),\n",
    "                                prior_strength=1., noise_std=0, \n",
    "                                cov_type='diagonal', shared_cov=True)\n",
    "hmm_shared_diag = beer.HMM.create(cgraph, modelset)\n",
    "\n",
    "# HMM (shared full cov).\n",
    "modelset = beer.NormalSet.create(data_mean, data_var, size=len(transitions),\n",
    "                                prior_strength=1., noise_std=0, \n",
    "                                cov_type='full', shared_cov=True)\n",
    "hmm_shared_full = beer.HMM.create(cgraph, modelset)\n",
    "\n",
    "\n",
    "models = {\n",
    "    'hmm_iso': hmm_iso.double(),\n",
    "    'hmm_diag': hmm_diag.double(), \n",
    "    'hmm_full': hmm_full.double(),\n",
    "    'hmm_shared_iso': hmm_shared_iso.double(),\n",
    "    'hmm_shared_diag': hmm_shared_diag.double(),\n",
    "    'hmm_shared_full': hmm_shared_full.double()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hmm_full) \n",
    "print(hmm_shared_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lrate = 1.\n",
    "X = torch.from_numpy(data).double()\n",
    "\n",
    "optims = {\n",
    "    model_name: beer.VariationalBayesOptimizer(model.mean_field_factorization(), lrate)\n",
    "    for model_name, model in models.items()\n",
    "}\n",
    "\n",
    "elbos = {\n",
    "    model_name: [] \n",
    "    for model_name in models\n",
    "}  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for name, model in models.items():\n",
    "        optim = optims[name]\n",
    "        optim.init_step()\n",
    "        elbo = beer.evidence_lower_bound(model, X, datasize=len(X), viterbi=False)\n",
    "        elbo.backward()\n",
    "        elbos[name].append(float(elbo) / len(X)) \n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    'hmm_iso': 'green',\n",
    "    'hmm_diag': 'blue',\n",
    "    'hmm_full': 'red',\n",
    "    'hmm_shared_iso': 'grey',\n",
    "    'hmm_shared_diag': 'brown',\n",
    "    'hmm_shared_full': 'black'\n",
    "    \n",
    "}\n",
    "# Plot the ELBO.\n",
    "fig = figure(title='ELBO', width=400, height=400, x_axis_label='step',\n",
    "              y_axis_label='ln p(X)')\n",
    "for model_name, elbo in elbos.items():\n",
    "    fig.line(range(len(elbo)), elbo, legend=model_name, color=colors[model_name])\n",
    "fig.legend.location = 'bottom_right'\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = []\n",
    "for i, model_name in enumerate(models):\n",
    "    model = models[model_name]\n",
    "    fig = figure(title=model_name, width=250, height=250)\n",
    "    fig.circle(data[:, 0], data[:, 1], alpha=.1)\n",
    "    plotting.plot_hmm(fig, model, alpha=.3, colors=['blue', 'red', 'green'])\n",
    "    if i % 3 == 0:\n",
    "        figs.append([])\n",
    "    figs[-1].append(fig)\n",
    "grid = gridplot(figs)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['hmm_full']\n",
    "best_path = model.decode(X).numpy()\n",
    "best_path, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['hmm_full']\n",
    "posts = model.posteriors(X).numpy()\n",
    "time_steps = range(len(X))\n",
    "\n",
    "fig = figure(width=800, height=400)\n",
    "fig.line(time_steps, posts[:, 0], color='blue')\n",
    "fig.line(time_steps, posts[:, 1], color='red')\n",
    "fig.line(time_steps, posts[:, 2], color='green')\n",
    "show(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
