{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Mixture Model\n",
    "\n",
    "This notebook illustrate how to build and train a Bayesian Hidden Markov Model with the [beer framework](https://github.com/beer-asr/beer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add \"beer\" to the PYTHONPATH\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import copy\n",
    "\n",
    "import beer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For plotting.\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "output_notebook()\n",
    "\n",
    "# Convenience functions for plotting.\n",
    "import plotting\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Generate data following HMM generative process\n",
    "#### Probability of initial states\n",
    "$$\n",
    "p(s^0 = s_1) = 1 \\\\\n",
    "p(s^0 = s_2) = 0 \\\\\n",
    "p(s^0 = s_3) = 0\n",
    "$$\n",
    "\n",
    "#### Probability of transitions\n",
    "$$\n",
    "p(s^t = s_1 \\vert s^{t-1} = s_1) = 0.5 \\quad p(s^t = s_2 \\vert s^{t-1} = s_1) = 0.5 \\quad p(s^t = s_3 \\vert s^{t-1} = s_1) = 0 \\\\\n",
    "p(s^t = s_1 \\vert s^{t-1} = s_2) = 0 \\quad p(s^t = s_2 \\vert s^{t-1} = s_2) = 0.5 \\quad p(s^t = s_3 \\vert s^{t-1} = s_2) = 0.5 \\\\\n",
    "p(s^t = s_1 \\vert s^{t-1} = s_3) = 0.5 \\quad p(s^t = s_2 \\vert s^{t-1} = s_3) = 0 \\quad p(s^t = s_3 \\vert s^{t-1} = s_3) = 0.5 \\\\  \n",
    "$$\n",
    "\n",
    "#### Emission\n",
    "$$\n",
    "p(x^t \\vert s^t = s_1) = \\mathcal{N}(x^t \\vert \\mu_1, \\Sigma_1) \\\\\n",
    "p(x^t \\vert s^t = s_2) = \\mathcal{N}(x^t \\vert \\mu_2, \\Sigma_2) \\\\\n",
    "p(x^t \\vert s^t = s_3) = \\mathcal{N}(x^t \\vert \\mu_3, \\Sigma_3)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "ndim = 2\n",
    "nstates = 3\n",
    "trans_mat = np.array([[.5, .5, 0], [0, .5, .5], [.5, 0, .5]])\n",
    "\n",
    "means = [np.array([-1.5, 4]),np.array([5, 5]), np.array([1, -2])]\n",
    "covs = [np.array([[.75, -.5], [-.5, 2.]]), np.array([[2, 1], [1, .75]]), np.array([[1, 0], [0, 1]]) ]\n",
    "normal_sets = [[means[0], covs[0]], [means[1], covs[1]], [means[2], covs[2]]] \n",
    "\n",
    "states = np.zeros(nsamples, dtype=np.int16)\n",
    "data = np.zeros((nsamples, ndim))\n",
    "states[0] = 0\n",
    "data[0] = np.random.multivariate_normal(means[states[0]], covs[states[0]], size=1)\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "fig1 = figure(title='Samples', width=400, height=400)\n",
    "fig1.circle(data[0, 0], data[0, 1], color=colors[states[0]])\n",
    "\n",
    "\n",
    "for n in range(1, nsamples):\n",
    "    states[n] = np.random.choice(np.arange(nstates), p=trans_mat[states[n-1]])\n",
    "    data[n] = np.random.multivariate_normal(means[states[n]], covs[states[n]], size=1)\n",
    "    fig1.circle(data[n, 0], data[n, 1], color=colors[states[n]], line_width=1)\n",
    "    fig1.line(data[n-1:n+1, 0], data[n-1:n+1, 1], color='black', line_width=.5, alpha=.5)\n",
    "\n",
    "fig2 = figure(title='Emissions',  width=400, height=400)\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, n in enumerate(normal_sets):\n",
    "    plotting.plot_normal(fig2, n[0], n[1], alpha=.3, color=colors[i])\n",
    "grid = gridplot([[fig1, fig2]])\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "We create several types of HMMs, each of them has the same transition matrix and initial / final state probability, and a specific type of emission density: \n",
    "  * one Normal density per state with full covariance matrix\n",
    "  * one Normal density per state with diagonal covariance matrix\n",
    "  * one Normal density per state with full covariance matrix shared across states\n",
    "  * one Normal density per state with diagonal covariance matrix shared across states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use the global mean/cov. matrix of the data to initialize the mixture.\n",
    "p_mean = torch.from_numpy(data.mean(axis=0)).float()\n",
    "p_cov = torch.from_numpy(np.cov(data.T)).float()\n",
    "\n",
    "init_states = torch.from_numpy(np.arange(nstates))\n",
    "final_states = torch.from_numpy(np.arange(nstates))\n",
    "trans_mat = torch.from_numpy(trans_mat).float()\n",
    "\n",
    "# HMM (diag cov).\n",
    "normalset = beer.NormalDiagonalCovarianceSet.create(p_mean, torch.diag(p_cov), \n",
    "                                                    nstates, noise_std=0.5)\n",
    "hmm_diag = beer.HMM.create(init_states, final_states, trans_mat, normalset)\n",
    "\n",
    "# HMM (full cov).\n",
    "normalset = beer.NormalFullCovarianceSet.create(p_mean, p_cov, nstates, \n",
    "                                                noise_std=0.5)\n",
    "hmm_full = beer.HMM.create(init_states, final_states, trans_mat, normalset)\n",
    "\n",
    "# HMM shared (full) cov.\n",
    "normalset = beer.NormalSetSharedDiagonalCovariance.create(p_mean, \n",
    "                                                        torch.diag(p_cov), \n",
    "                                                        nstates,\n",
    "                                                        noise_std=0.5)\n",
    "hmm_sharedcov_diag = beer.HMM.create(init_states, final_states, trans_mat, normalset)\n",
    "\n",
    "# HMM shared (full) cov.\n",
    "normalset = beer.NormalSetSharedFullCovariance.create(p_mean, p_cov, nstates,\n",
    "                                                      noise_std=0.5)\n",
    "hmm_sharedcov_full = beer.HMM.create(init_states, final_states, trans_mat, normalset)\n",
    "\n",
    "models = [\n",
    "    hmm_diag, \n",
    "    hmm_full,\n",
    "    hmm_sharedcov_diag,\n",
    "    hmm_sharedcov_full\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "lrate = 1.\n",
    "labels = states\n",
    "X = torch.from_numpy(data).float()\n",
    "#Z = torch.from_numpy(labels).long()\n",
    "Z = None\n",
    "elbo_fn = beer.EvidenceLowerBound(len(X))\n",
    "params = []\n",
    "for model in models:\n",
    "    params += model.parameters\n",
    "optimizer = beer.BayesianModelOptimizer(params, lrate)\n",
    "    \n",
    "elbos = [[], [], [], []]\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for i, model in enumerate(models):\n",
    "        elbo = elbo_fn(model, X, Z)\n",
    "        elbo.natural_backward()\n",
    "        elbos[i].append(float(elbo) / len(X))\n",
    "    optimizer.step()\n",
    "\n",
    "# Plot the ELBO.\n",
    "fig = figure(title='ELBO', width=400, height=400, x_axis_label='step',\n",
    "              y_axis_label='ln p(X)')\n",
    "fig.line(range(epochs), elbos[0], legend='HMM (diag)', color='blue')\n",
    "fig.line(range(epochs), elbos[1], legend='HMM (full)', color='red')\n",
    "fig.line(range(epochs), elbos[2], legend='HMM (shared cov. diag)', color='green')\n",
    "fig.line(range(epochs), elbos[3], legend='HMM (shared cov. full)', color='black')\n",
    "fig.legend.location = 'bottom_right'\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean = data.mean(axis=0)\n",
    "var = data.var(axis=0)\n",
    "std_dev = np.sqrt(max(var))\n",
    "x_range = (mean[0] - 2 * std_dev, mean[0] + 2 * std_dev)\n",
    "y_range = (mean[1] - 2 * std_dev, mean[1] + 2 * std_dev)\n",
    "global_range = (min(x_range[0], y_range[0]), max(x_range[1], y_range[1]))\n",
    "\n",
    "fig1 = figure(title='HMM (diag)', x_range=global_range, y_range=global_range,\n",
    "              width=400, height=400)\n",
    "fig1.circle(data[:, 0], data[:, 1], alpha=.1)\n",
    "plotting.plot_hmm(fig1, hmm_diag, alpha=.1, color='blue')\n",
    "\n",
    "fig2 = figure(title='HMM (full)', x_range=global_range, y_range=global_range,\n",
    "              width=400, height=400)\n",
    "fig2.circle(data[:, 0], data[:, 1], alpha=.1)\n",
    "plotting.plot_hmm(fig2, hmm_full, alpha=.1, color='red')\n",
    "\n",
    "fig3 = figure(title='HMM (shared cov. diag)', x_range=global_range, y_range=global_range,\n",
    "              width=400, height=400)\n",
    "fig3.circle(data[:, 0], data[:, 1], alpha=.1)\n",
    "plotting.plot_hmm(fig3, hmm_sharedcov_diag, alpha=.1, color='green')\n",
    "\n",
    "fig4 = figure(title='HMM (shared cov. full)', x_range=global_range, y_range=global_range,\n",
    "              width=400, height=400)\n",
    "fig4.circle(data[:, 0], data[:, 1], alpha=.1)\n",
    "plotting.plot_hmm(fig4, hmm_sharedcov_full, alpha=.1, color='black')\n",
    "\n",
    "grid = gridplot([[fig1, fig2], [fig3, fig4]])\n",
    "show(grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
