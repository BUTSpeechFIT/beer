{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Variational AutoEncoder\n",
    "\n",
    "This notebook illustrate how to build and train a Structured Variational AutoEncoder (SVAE) with the [beer framework](https://github.com/beer-asr/beer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add the path of the beer source code ot the PYTHONPATH.\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# For plotting.\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "# Beer framework\n",
    "import beer\n",
    "\n",
    "# Convenience functions for plotting.\n",
    "import plotting\n",
    "\n",
    "output_notebook(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "As a simple example we consider the following synthetic data: \n",
    "\n",
    "$$ \n",
    "\\begin{split}\n",
    "    z &\\sim \\mathcal{N}(m, \\Sigma) \\\\\n",
    "    x &= \n",
    "        \\begin{pmatrix}\n",
    "        z_1 \\\\\n",
    "        z_2 + (z_1 - m_1)^2\n",
    "        \\end{pmatrix} \n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate some Normal distributed samples.\n",
    "mean = np.array([3., 2.])\n",
    "cov = np.array([[2., 1.], [1., .75]])\n",
    "Z = np.random.multivariate_normal(mean, cov, size=100)\n",
    "\n",
    "# Apply the non-linear transformation.\n",
    "X = np.zeros_like(Z)\n",
    "X[:, 0] = Z[:, 0]\n",
    "X[:, 1] = Z[:, 1] + (Z[:, 0]-mean[0])**2\n",
    "\n",
    "fig = figure(title='Synthetic data', width=400, height=400)\n",
    "fig.circle(X[:, 0], X[:, 1])\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features normalization\n",
    "\n",
    "Since the VAE model is built upon neural network components, it is a good practice to mean-variance normalize the features to ease up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = X.mean(axis=0)\n",
    "data_cov = np.cov(X.T)\n",
    "X -= data_mean\n",
    "X /= np.sqrt(np.diag(data_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "We first create the SVAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the observed space.\n",
    "obs_dim = X.shape[1]\n",
    "\n",
    "# Dimension of the latent space. It can be bigger or smaller\n",
    "# than the dimension of the observed space.\n",
    "latent_dim = 2\n",
    "\n",
    "# Number of samples for the \"reparameterization-trick\".\n",
    "nb_samples = 10\n",
    "\n",
    "# Number of units per hidden-layer.\n",
    "n_units = 10\n",
    "\n",
    "# beer uses pytorch as a backend for the neural-network part\n",
    "# of the model.\n",
    "from torch import nn\n",
    "\n",
    "# Neural network structure of the encoder of the model.\n",
    "enc_struct = nn.Sequential(\n",
    "    nn.Linear(obs_dim, n_units),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_units, n_units),\n",
    "    nn.Tanh()\n",
    ")\n",
    "encoder = beer.models.MLPNormalIso(enc_struct, latent_dim, residual=True)\n",
    "\n",
    "# Neural network structure of the decoder of the model.\n",
    "dec_struct = nn.Sequential(\n",
    "    nn.Linear(latent_dim, n_units),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_units, n_units),\n",
    "    nn.Tanh()\n",
    ")\n",
    "decoder = beer.models.MLPNormalDiag(dec_struct, obs_dim)\n",
    "\n",
    "# Model of the latent space (uncomment the one you want to try).\n",
    "# It can be changed at any-time. \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "latent_model = beer.models.NormalDiagonalCovariance.create(latent_dim)\n",
    "\n",
    "#latent_model = beer.models.NormalFullCovariance.create(latent_dim, prior_count=1)\n",
    "\n",
    "#args = {'dim':2, 'prior_count':1, 'mean': data_mean, 'cov': data_cov, 'random_init':True}\n",
    "#latent_model = beer.Mixture.create(10, beer.NormalDiagonalCovariance.create, args, prior_count=1e-6)\n",
    "\n",
    "#args = {'dim':2, 'prior_count':1, 'mean': data_mean, 'cov': data_cov, 'random_init':True}\n",
    "#latent_model = beer.Mixture.create(10, beer.NormalFullCovariance.create, args, prior_count=1)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Putting everything together to build the SVAE.\n",
    "svae = beer.models.VAE(encoder, decoder, latent_model, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Bayes Inference is sensitive to the initialization of the posterior. Our initialization scheme is fairly basic but seems to provide good results on this toy example:\n",
    "  1. Keep the prior fixed and optimize the expected value of the  log-likelihood of the VAE (i.e. loss function without the KL divergence term).\n",
    "  2. Freeze the parameters of the encoder/decoder and update the latent model so it fits the current distribution of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svae.fit(X, max_epochs=500, lrate=1e-3, latent_model_lrate=0, kl_weight=0.0, callback=None)\n",
    "svae.fit(X, max_epochs=500, lrate=0, latent_model_lrate=1e-1, kl_weight=0.0, callback=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Bayes Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to monitor the training progress.\n",
    "elbos, llhs, klds = [], [], []\n",
    "def callback(elbo, llh, kld):\n",
    "    elbos.append(elbo)\n",
    "    llhs.append(llh)\n",
    "    klds.append(kld)\\\n",
    "\n",
    "# This is the training.\n",
    "svae.fit(X, max_epochs=10000, lrate=1e-3, latent_model_lrate=1e-2, callback=callback)\n",
    "\n",
    "# Plot the ELBO.\n",
    "fig1 = figure(title='ELBO', width=400, height=400, x_axis_label='step',\n",
    "              y_axis_label='ln p(X)')\n",
    "fig1.line(np.arange(len(elbos)), elbos)\n",
    "\n",
    "# Plot the LLH and the KLD separately.\n",
    "fig2 = figure(title='LLH + KLD', width=400, height=400,\n",
    "              y_range=(min(llhs) - 1, max(llhs) + 1),\n",
    "              x_axis_label='step', y_axis_label='ln p(x|...)')\n",
    "fig2.line(np.arange(len(llhs)), llhs)\n",
    "fig2.extra_y_ranges['KLD'] = Range1d(0, max(klds) + 1)\n",
    "fig2.add_layout(LinearAxis(y_range_name=\"KLD\", axis_label='KLD'), 'right')\n",
    "fig2.line(np.arange(len(klds)), klds, y_range_name='KLD', color='green')\n",
    "\n",
    "show(gridplot([[fig1, fig2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the VAE has learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = 100\n",
    "x_range = (-3, 3)\n",
    "y_range = (-3, 3)\n",
    "xy = np.mgrid[x_range[0]:x_range[1]:100j, y_range[0]:y_range[1]:100j].reshape(2,-1).T\n",
    "elbo, llh, kld, mean, var = svae.evaluate(xy, sampling=False)\n",
    "fig1 = figure(x_range=x_range,  y_range=y_range, width=400, height=400)\n",
    "\n",
    "# must give a vector of image data for image parameter\n",
    "fig1.image(image=[np.exp((elbo).reshape(d, d).T)], \n",
    "           x=x_range[0], y=y_range[0], \n",
    "           dw=(x_range[1] - x_range[0]), dh=(y_range[1] - y_range[0]),\n",
    "           palette=\"Inferno256\", alpha=.01)\n",
    "fig1.circle(X[:, 0], X[:, 1], alpha=1)\n",
    "\n",
    "fig2 = figure(width=400, height=400)\n",
    "\n",
    "elbo, llh, kld, mean, var = svae.evaluate(X[:100], sampling=False)\n",
    "fig2.cross(mean[:, 0], mean[:, 1], color='black')\n",
    "for m, v in zip(mean, var):\n",
    "    fig2.ellipse(x=m[0], y=m[1], \n",
    "                 width=2 * np.sqrt(v[0]), \n",
    "                 height=2 * np.sqrt(v[1]), \n",
    "                 fill_alpha=0, color='black') \n",
    "plotting.plot_latent_model(fig2, svae.latent_model, alpha=.5, color='salmon')\n",
    "\n",
    "grid = gridplot([[fig1, fig2]])\n",
    "show(grid)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
