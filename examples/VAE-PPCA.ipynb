{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder\n",
    "\n",
    "This notebook illustrate how to build and train a Variation AutoEncoder with the [beer framework](https://github.com/beer-asr/beer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"beer\" to the PYTHONPATH\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import copy\n",
    "\n",
    "import beer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# For plotting.\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "output_notebook()\n",
    "\n",
    "# Convenience functions for plotting.\n",
    "import plotting\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "As an illustration, we generate a synthetic data set composed of two Normal distributed cluster.\n",
    "One has a diagonal covariance matrix whereas the other has a dense covariance matrix.\n",
    "Those two clusters overlap so it is reasonable to map all the data to a single Gaussian in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cluster.\n",
    "mean = np.array([3, 3]) \n",
    "x1 = np.random.randn(100) * 1.5\n",
    "x2 = np.power(.75 * x1, 2)\n",
    "x = np.c_[x1, x2]\n",
    "data = mean + x + .2 * np.random.randn(len(x), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean, variance of the data to scale the figure.\n",
    "mean = data.mean(axis=0)\n",
    "var = data.var(axis=0)\n",
    "std_dev = np.sqrt(max(var))\n",
    "x_range = (mean[0] - 5 * std_dev, mean[0] + 5 * std_dev)\n",
    "y_range = (mean[1] - 5 * std_dev, mean[1] + 5 * std_dev)\n",
    "global_range = (min(x_range[0], y_range[0]), max(x_range[1], y_range[1]))\n",
    "\n",
    "fig = figure(title='Data', width=400, height=400,\n",
    "             x_range=global_range, y_range=global_range)\n",
    "fig.circle(data[:, 0], data[:, 1])\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(9)\n",
    "observed_dim = 2\n",
    "latent_dim1 = 2\n",
    "latent_dim2 = 1\n",
    "latent_normal = beer.PPCA.create(\n",
    "    torch.zeros(latent_dim1), 1., torch.eye(latent_dim2, latent_dim1)\n",
    ")\n",
    "\n",
    "# Number of units per hidden-layer.\n",
    "n_units = 20\n",
    "\n",
    "class GaussianMLP(nn.Module):\n",
    "    def __init__(self, structure, space_dim, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.nn = structure\n",
    "        self.h2mean = nn.Linear(n_units, space_dim)\n",
    "        self.h2logvar = nn.Linear(n_units, space_dim)\n",
    "    \n",
    "        self.h2logvar.bias.data += -1.0 # init with small (log)variance\n",
    "            \n",
    "    def forward(self, X):\n",
    "        h = self.nn(X)\n",
    "        mean = self.h2mean(h)\n",
    "        if self.residual:\n",
    "            mean += X\n",
    "        logvar = self.h2logvar(h)\n",
    "        \n",
    "        return beer.NormalDiagonalCovarianceMLP(mean, logvar.exp())\n",
    "\n",
    "    \n",
    "# Neural network structure of the encoder/decoder of the model.\n",
    "enc_struct = nn.Sequential(\n",
    "    nn.Linear(observed_dim, n_units),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_units, n_units),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "\n",
    "dec_struct = nn.Sequential(\n",
    "    nn.Linear(observed_dim, n_units),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(n_units, n_units),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "\n",
    "#latent_normal = beer.NormalDiagonalCovariance.create(\n",
    "#    torch.zeros(latent_dim1), torch.ones(latent_dim1)\n",
    "#)\n",
    "vae_ppca = beer.VAE(\n",
    "    GaussianMLP(enc_struct, observed_dim, residual=True), \n",
    "    GaussianMLP(dec_struct, observed_dim, residual=True), \n",
    "    latent_normal, \n",
    "    nsamples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5_000\n",
    "lrate_bayesmodel = 1.\n",
    "lrate_encoder = 1e-3\n",
    "X = torch.from_numpy(data).float()\n",
    "elbo_fn = beer.EvidenceLowerBound(len(X))\n",
    "\n",
    "nnet_parameters = list(vae_ppca.encoder.parameters()) + list(vae_ppca.decoder.parameters())\n",
    "std_optimizer = torch.optim.Adam(nnet_parameters, lr=lrate_encoder, weight_decay=1e-2)\n",
    "params = vae_ppca.latent_model.parameters\n",
    "optim = beer.BayesianModelOptimizer(params,lrate=lrate_bayesmodel, \n",
    "                                    std_optim=std_optimizer)\n",
    "    \n",
    "elbos, klds, llhs = [], [], []\n",
    "for epoch in range(epochs):\n",
    "    optim.zero_grad()\n",
    "    elbo = elbo_fn(vae_ppca, X)\n",
    "    elbo.backward()\n",
    "    elbo.natural_backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if epoch > 0:\n",
    "        elbos.append(float(elbo) / len(X))\n",
    "        llhs.append(float(elbo.expected_llh) / len(X))\n",
    "        klds.append(float(elbo.kl_div) / len(X))\n",
    "\n",
    "fig1 = figure(title='ELBO', width=400, height=400, x_axis_label='step',\n",
    "              y_axis_label='ln p(X)')\n",
    "fig1.line(np.arange(len(elbos)), elbos, color='blue')\n",
    "\n",
    "fig2 = figure(title='LLH', width=400, height=400, x_axis_label='step',\n",
    "              y_axis_label='ln p(X)')\n",
    "fig2.line(np.arange(len(elbos)), llhs, color='green')\n",
    "\n",
    "fig3 = figure(title='KL', width=400, height=400, x_axis_label='step',\n",
    "              y_axis_label='ln p(X)')\n",
    "fig3.line(np.arange(len(elbos)), klds, color='red')\n",
    "\n",
    "show(gridplot([[fig1], [fig2, fig3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vae_ppca.latent_model\n",
    "model_mean = model.mean.numpy()\n",
    "S = model.subspace.numpy()\n",
    "model_cov = S.T @ S + np.identity(2) / model.precision.numpy()\n",
    "\n",
    "def transform(xy):\n",
    "    dec_state = vae_ppca.decoder(model_mean + torch.from_numpy(xy).float())\n",
    "    return dec_state.mean.detach().numpy()\n",
    "\n",
    "def transform2(xy):\n",
    "    dec_state = vae_ppca.decoder(model_mean + torch.from_numpy(xy).float())\n",
    "    return dec_state.mean.detach().numpy(), \\\n",
    "        dec_state.var.detach().numpy()\n",
    "\n",
    "A, B = model.mean.numpy(), model.mean.numpy() + model.subspace.numpy()[0, :]\n",
    "slope = (A[1] - B[1]) / (A[0] - B[0])\n",
    "intercept = -slope * ((slope * A[0] - A[1]) / slope)\n",
    "x = np.linspace(-20, 20, 1000)\n",
    "o_s_line = np.c_[x, slope * x + intercept]\n",
    "xy = transform(o_s_line)\n",
    "s_line = np.c_[x, slope * x + intercept + np.sqrt(1/model.precision.numpy())]\n",
    "confidence1 = transform(s_line)\n",
    "s_line = np.c_[x, slope * x + intercept - np.sqrt(1/model.precision.numpy())]\n",
    "confidence2 = transform(s_line)\n",
    "s_line = np.c_[x, slope * x + intercept + 2 * np.sqrt(1/model.precision.numpy())]\n",
    "confidence3 = transform(s_line)\n",
    "s_line = np.c_[x, slope * x + intercept - 2 * np.sqrt(1/model.precision.numpy())]\n",
    "confidence4 = transform(s_line)\n",
    "    \n",
    "fig1 = figure(plot_width=400, plot_height=400, x_range=(-10, 10),\n",
    "             y_range=(0, 15))\n",
    "fig1.line(xy[:, 0], xy[:, 1])\n",
    "fig1.circle(data[:, 0], data[:, 1], color='red', alpha=.5)\n",
    "means, variances = transform2(o_s_line)\n",
    "print(means.shape, variances.shape)\n",
    "for i in range(len(means)):\n",
    "    mean = means[i]\n",
    "    cov = np.diag(variances[i])\n",
    "    plotting.plot_normal(fig1, mean, cov, n_std_dev=2, alpha=.1)\n",
    "show(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vae_ppca.latent_model\n",
    "enc_state = vae_ppca.encoder(X)\n",
    "means, dcovs = enc_state.mean.detach().numpy(), enc_state.var.detach().numpy()\n",
    "\n",
    "x = np.linspace(-20, 20, 1000)\n",
    "\n",
    "A, B = model.mean.numpy(), model.mean.numpy() + model.subspace.numpy()[0, :]\n",
    "slope = (A[1] - B[1]) / (A[0] - B[0])\n",
    "intercept = -slope * ((slope * A[0] - A[1]) / slope)\n",
    "s_line = np.c_[x, slope * x + intercept]\n",
    "p_h = np.sqrt(1 / (2 * np.pi)) * np.exp(-.5 * x ** 2)\n",
    "angle = np.arctan(abs(B[1] - A[1]) / abs(B[0] - A[0]))\n",
    "R = np.array([\n",
    "    [np.cos(angle), -np.sin(angle)],\n",
    "    [np.sin(angle), np.cos(angle)]\n",
    "])\n",
    "\n",
    "fig1 = figure(plot_width=400, plot_height=400, x_range=(-5, 5),\n",
    "             y_range=(-5, 5))\n",
    "\n",
    "for i in range(2):\n",
    "    xy = np.c_[x, np.zeros_like(x)]\n",
    "    rxy1 = xy @ R.T + model.mean.numpy()\n",
    "    xy = np.c_[x, (i + 1) * np.sqrt(np.ones_like(x) / model.precision.numpy())]\n",
    "    rxy2 = xy @ R.T + model.mean.numpy()\n",
    "    band_x = np.append(rxy1[:,0], rxy2[:, 0][::-1])\n",
    "    band_y = np.append(rxy1[:,1], rxy2[:, 1][::-1])\n",
    "    fig1.patch(band_x, band_y, line_alpha=0., fill_alpha=0.3, fill_color='LightBlue')\n",
    "\n",
    "    xy = np.c_[x, np.zeros_like(x)]\n",
    "    rxy1 = xy @ R.T + model.mean.numpy()\n",
    "    xy = np.c_[x, -(i + 1) * np.sqrt(np.ones_like(x) / model.precision.numpy())]\n",
    "    rxy2 = xy @ R.T + model.mean.numpy()\n",
    "    band_x = np.append(rxy1[:,0], rxy2[:, 0][::-1])\n",
    "    band_y = np.append(rxy1[:,1], rxy2[:, 1][::-1])\n",
    "    fig1.patch(band_x, band_y, line_alpha=0., fill_alpha=0.3, fill_color='LightBlue')\n",
    "\n",
    "fig1.cross(means[:30, 0], means[:30, 1], color='red', alpha=.5)\n",
    "for i in range(30):\n",
    "    mean = means[i]\n",
    "    cov = np.diag(dcovs[i])\n",
    "    plotting.plot_normal(fig1, mean, cov, n_std_dev=2, line_alpha=.0, \n",
    "                         line_color='black', fill_alpha=.3, fill_color='red')\n",
    "    \n",
    "xy = np.c_[x, np.zeros_like(x)]\n",
    "rxy1 = xy @ R.T + model.mean.numpy() \n",
    "xy = np.c_[x, p_h]\n",
    "rxy2 = xy @ R.T + model.mean.numpy()\n",
    "band_x = np.append(rxy1[:,0], rxy2[:, 0][::-1])\n",
    "band_y = np.append(rxy1[:,1], rxy2[:, 1][::-1])\n",
    "fig1.patch(band_x, band_y, line_color='black', fill_color='LightGreen', alpha=.5)\n",
    "\n",
    "show(fig1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
