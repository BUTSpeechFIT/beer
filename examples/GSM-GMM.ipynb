{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Subspace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"beer\" to the PYTHONPATH\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import beer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For plotting.\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, gridplot\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "from bokeh.palettes import Category10 as palette\n",
    "output_notebook()\n",
    "\n",
    "# Convenience functions for plotting.\n",
    "import plotting\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(global_mean, angle, size, weight):\n",
    "    rotation = np.array([\n",
    "        [math.cos(angle), -math.sin(angle)],\n",
    "        [math.sin(angle), math.cos(angle)]\n",
    "    ])\n",
    "    scale = np.array([.5, 2])\n",
    "    init_cov = np.diag(scale**2)\n",
    "    cov1 = rotation.T @ init_cov @ rotation\n",
    "    cov2 = rotation @ init_cov @ rotation.T\n",
    "    mean1 = global_mean + np.array([0, 3])\n",
    "    mean2 = global_mean - np.array([0, 3])\n",
    "    data1 = (scale * np.random.randn(int(size * weight), 2)) @ rotation + mean1 \n",
    "    data2 = (scale * np.random.randn(int(size * (1 - weight)), 2)) @ rotation.T + mean2 \n",
    "    data = np.vstack([data1, data2])\n",
    "    np.random.shuffle(data)\n",
    "    return data, (mean1, mean2), (cov1, cov2), (weight, 1-weight)\n",
    "\n",
    "datasets = []\n",
    "means = []\n",
    "covs = []\n",
    "weights = []\n",
    "start_angle = -.5 * math.pi\n",
    "boundary = 50\n",
    "nmodels = 10\n",
    "for h in np.linspace(-boundary, boundary, nmodels):\n",
    "    mean = np.array([1., 0]) * h\n",
    "    ratio = (h + boundary) / (2 * boundary)\n",
    "    angle = start_angle + ratio * (math.pi)\n",
    "    w_ratio = .1 + .8 * ratio\n",
    "    data, m_means, m_covs, m_weights = generate_data(mean, angle, size=200, weight=w_ratio)\n",
    "    datasets.append(data)\n",
    "    means.append(m_means)\n",
    "    covs.append(m_covs)\n",
    "    weights.append(m_weights)\n",
    "data = np.vstack(datasets)\n",
    "\n",
    "# Convert the data to pytorch tensor to work with beer.  \n",
    "datasets = [torch.from_numpy(data) for data in datasets]\n",
    "\n",
    "# Colors \n",
    "colors = palette[10] * 2\n",
    "\n",
    "fig = figure()\n",
    "for color, dataset, m_means, m_covs, m_weights in zip(colors, datasets, means, covs, weights):\n",
    "    dataset = dataset.numpy()\n",
    "    plotting.plot_normal(fig, m_means[0], m_covs[0], alpha=.5 * m_weights[0], color=color)\n",
    "    plotting.plot_normal(fig, m_means[1], m_covs[1], alpha=.5 * m_weights[1], color=color)\n",
    "    fig.cross(m_means[0][0], m_means[0][1], color=color, size=7, line_width=2)\n",
    "    fig.cross(m_means[1][0], m_means[1][1], color=color, size=7, line_width=2)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Subspace Model\n",
    "\n",
    "### Creating the GSM\n",
    "\n",
    "The GSM is composed of a latent prior, an affine transformation, a generic subspace model which indicates how to transform the projections of the embedding into a concrete model and the instances of the generic subspace model (paired with latent posterior distributions, one for each subspace model instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 2\n",
    "latent_dim = 2\n",
    "\n",
    "# Type of covariance for the Subspace GMMs.\n",
    "cov_type = 'full' # full/diagonal/isotropic\n",
    " \n",
    "# Prior over the latent space.\n",
    "latent_prior = beer.Normal.create(\n",
    "    torch.zeros(latent_dim), \n",
    "    torch.ones(latent_dim),\n",
    "    prior_strength=1e-3\n",
    ").double()\n",
    "\n",
    "# Data model (SGMM).\n",
    "modelset = beer.NormalSet.create(\n",
    "    mean=torch.zeros(obs_dim), cov=torch.ones(obs_dim),\n",
    "    size=2,\n",
    "    cov_type=cov_type\n",
    ")\n",
    "sgmm = beer.Mixture.create(modelset).double()\n",
    "\n",
    "# We specify which parameters will be handled by the\n",
    "# subspace in the GMM. \n",
    "newparams = {\n",
    "    param: beer.SubspaceBayesianParameter.from_parameter(param, latent_prior)\n",
    "    for param in sgmm.bayesian_parameters()\n",
    "}\n",
    "sgmm.replace_parameters(newparams)\n",
    "\n",
    "# Create the Generalized Subspace Model\n",
    "gsm = beer.GSM.create(sgmm, latent_dim, latent_prior, prior_strength=1e-3).double()\n",
    "\n",
    "# Create the instance of SGMM for each dataset\n",
    "sgmms, latent_posts = gsm.new_models(len(datasets), cov_type='diagonal')\n",
    "\n",
    "print('Latent prior')\n",
    "print('============')\n",
    "print(latent_prior)    \n",
    "print()\n",
    "\n",
    "print('Subspace GMM (generic model)')\n",
    "print('============================')\n",
    "print(sgmm)    \n",
    "print()\n",
    "\n",
    "print('Generalized Subspace Model')\n",
    "print('==========================')\n",
    "print(gsm) \n",
    "print()\n",
    "\n",
    "print('Subspace GMMs (concrete instances)')\n",
    "print('==================================')\n",
    "print('(1) -', sgmms[0])   \n",
    "print()\n",
    "print('...')\n",
    "print()\n",
    "print(f'({len(datasets)}) -', sgmms[-1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training\n",
    "\n",
    "Before to start the training we need to inialize the subspace. To do so, we first train a Normal distribution for each dataset and we'll use its statistics as initial statistics for all the Normal distributions of the SGMMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normal(dataset, cov_type):\n",
    "    data_mean = dataset.mean(dim=0)\n",
    "    data_var = dataset.var(dim=0)\n",
    "    return beer.Normal.create(data_mean, data_var, cov_type=cov_type).double()\n",
    "\n",
    "def fit_normal(normal, dataset, epochs=100):\n",
    "    optim = beer.VBConjugateOptimizer(normal.mean_field_factorization(), lrate=1.)\n",
    "    for epoch in range(epochs):\n",
    "        optim.init_step()\n",
    "        elbo = beer.evidence_lower_bound(normal, dataset)\n",
    "        elbo.backward()\n",
    "        optim.step()\n",
    "        \n",
    "normals = [create_normal(dataset, cov_type=cov_type) for dataset in datasets]\n",
    "for normal, dataset in zip(normals, datasets):\n",
    "    fit_normal(normal, dataset)\n",
    "    \n",
    "fig = figure(width=400, height=400)\n",
    "for normal, dataset, color in zip(normals, datasets, colors):\n",
    "    dataset = dataset.numpy()\n",
    "    mean = normal.mean.numpy()\n",
    "    cov = normal.cov.numpy()\n",
    "    plotting.plot_normal(fig, mean, cov, alpha=.5, color=color)\n",
    "    fig.circle(dataset[:, 0], dataset[:, 1], color=color, alpha=.1)\n",
    "    \n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial weights sufficient statistics.\n",
    "ncomp = len(sgmm.modelset)\n",
    "weights_stats = torch.zeros(len(datasets), ncomp).double()\n",
    "counts = torch.cat([torch.tensor(float(len(dataset))).view(1) for dataset in datasets]).double()\n",
    "weights_stats[:] = counts[:, None] / ncomp\n",
    "weights_stats[:, -1] = counts\n",
    "weights_stats\n",
    "\n",
    "# Prepare the initial sufficient statistics for the \n",
    "# components of the GMM.\n",
    "normals_stats =  [normal.mean_precision.stats.repeat(ncomp, 1)\n",
    "                  for normal in normals]\n",
    "for i, gmm in enumerate(sgmms):\n",
    "    gmm.categorical.weights.stats = weights_stats[i]\n",
    "    gmm.modelset.means_precisions.stats = normals_stats[i]\n",
    "    \n",
    "# NOTE: we initialize the stats of all the parameters\n",
    "# whether they are included in the subspace or not.\n",
    "# For parameters that are not included in the subspace,\n",
    "# this initialization will be discarded during\n",
    "# the training (\"optim.init_step()\" clear the stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15_000\n",
    "params = gsm.conjugate_bayesian_parameters(keepgroups=True)\n",
    "cjg_optim = beer.VBConjugateOptimizer(params, lrate=1.)\n",
    "params = list(latent_posts.parameters()) + list(gsm.parameters())\n",
    "std_optim = torch.optim.Adam(params, lr=1e-1)\n",
    "optim = beer.VBOptimizer(cjg_optim, std_optim)\n",
    "\n",
    "elbos = []\n",
    "for epoch in range(1, epochs + 1): \n",
    "    optim.init_step()\n",
    "    elbo = beer.evidence_lower_bound(gsm, sgmms, latent_posts=latent_posts, \n",
    "                                     latent_nsamples=5, params_nsamples=5)\n",
    "    elbo.backward()\n",
    "    optim.step()\n",
    "    elbos.append(float(elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure()\n",
    "fig.line(range(len(elbos)), elbos)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = figure(title='True model')\n",
    "for color, dataset, m_means, m_covs, m_weights in zip(colors, datasets, means, covs, weights):\n",
    "    dataset = dataset.numpy()\n",
    "    plotting.plot_normal(fig1, m_means[0], m_covs[0], alpha=.7 * m_weights[0], color=color)\n",
    "    plotting.plot_normal(fig1, m_means[1], m_covs[1], alpha=.7 * m_weights[1], color=color)\n",
    "    fig1.circle(dataset[:, 0], dataset[:, 1], alpha=.5, color=color)\n",
    "    \n",
    "fig2 = figure(title='Subspace GMM', x_range=fig1.x_range, y_range=fig1.y_range)\n",
    "for gmm, dataset, color in zip(sgmms, datasets, colors):\n",
    "    dataset = dataset.numpy()\n",
    "    plotting.plot_gmm(fig2, gmm, alpha=.7, color=color)\n",
    "    fig2.circle(dataset[:, 0], dataset[:, 1], color=color, alpha=.5)\n",
    "    \n",
    "fig3 = figure(title='Latent space')\n",
    "mean, cov = gsm.latent_prior.mean.numpy(), gsm.latent_prior.cov.numpy()\n",
    "plotting.plot_normal(fig3, mean, cov, alpha=.5, color='pink')\n",
    "for mean, cov, color in zip(latent_posts.params.mean, latent_posts.params.diag_cov, colors):\n",
    "    mean = mean.detach().numpy()\n",
    "    cov = cov.diag().detach().numpy()\n",
    "    plotting.plot_normal(fig3, mean, cov, alpha=.5, color=color)\n",
    "\n",
    "\n",
    "show(gridplot([[fig1, fig2], [None, fig3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now the GSM is initialized so we start the \"actual\" training by updating the statistics of the parameters whenever the GSM is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5_000\n",
    "stats_update_rate = 100\n",
    "\n",
    "# This function accumulate the statistics for the parameters\n",
    "# of the subspace and update the parameters that are not\n",
    "# part of the subspace.\n",
    "def accumulate_stats(models, datasets, optims):\n",
    "    for model, X, optim in zip(models, datasets, optims):\n",
    "        optim.init_step()\n",
    "        elbo = beer.evidence_lower_bound(model, X) \n",
    "        elbo.backward(std_params=False)\n",
    "        optim.step()\n",
    "        \n",
    "# Prepare an optimzer for each SGMM. The optimizer\n",
    "# will handle all parameters that are note included\n",
    "# in the subspace.\n",
    "sgmms_optims = []\n",
    "for gmm in sgmms:\n",
    "    pfilter = lambda param: not isinstance(param, beer.SubspaceBayesianParameter)\n",
    "    params = gmm.bayesian_parameters(\n",
    "        paramtype=beer.ConjugateBayesianParameter,\n",
    "        paramfilter=pfilter,\n",
    "        keepgroups=True\n",
    "    )\n",
    "    soptim = beer.VBConjugateOptimizer(params, lrate=1.)\n",
    "    sgmms_optims.append(soptim)\n",
    "    \n",
    "\n",
    "elbos = []\n",
    "for epoch in range(1, epochs + 1): \n",
    "    if (epoch - 1) % stats_update_rate == 0:\n",
    "        accumulate_stats(sgmms, datasets, sgmms_optims)\n",
    "    optim.init_step()\n",
    "    elbo = beer.evidence_lower_bound(gsm, sgmms, latent_posts=latent_posts, \n",
    "                                     latent_nsamples=10, params_nsamples=10)\n",
    "    elbo.backward()\n",
    "    optim.step()\n",
    "    elbos.append(float(elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure()\n",
    "fig.line(range(len(elbos)), elbos)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = figure(title='True model', width=600, height=600)\n",
    "for color, dataset, m_means, m_covs, m_weights in zip(colors, datasets, means, covs, weights):\n",
    "    dataset = dataset.numpy()\n",
    "    plotting.plot_normal(fig1, m_means[0], m_covs[0], alpha=.8 * m_weights[0], color=color)\n",
    "    plotting.plot_normal(fig1, m_means[1], m_covs[1], alpha=.8 * m_weights[1], color=color)\n",
    "    fig1.circle(dataset[:, 0], dataset[:, 1], alpha=.5, color=color)\n",
    "    \n",
    "fig2 = figure(title='Subspace GMM', x_range=fig1.x_range, y_range=fig1.y_range, width=600, height=600)\n",
    "for gmm, dataset, color in zip(sgmms, datasets, colors):\n",
    "    dataset = dataset.numpy()\n",
    "    plotting.plot_gmm(fig2, gmm, alpha=.8, color=color)\n",
    "    fig2.circle(dataset[:, 0], dataset[:, 1], color=color, alpha=.5)\n",
    "    \n",
    "fig3 = figure(title='Latent space', width=600, height=600)\n",
    "mean, cov = gsm.latent_prior.mean.numpy(), gsm.latent_prior.cov.numpy()\n",
    "plotting.plot_normal(fig3, mean, cov, alpha=.5, color='pink')\n",
    "#fig3.x_range = fig3.y_range\n",
    "for mean, cov, color in zip(latent_posts.params.mean, latent_posts.params.diag_cov, colors):\n",
    "    mean = mean.detach().numpy()\n",
    "    cov = cov.diag().detach().numpy()\n",
    "    plotting.plot_normal(fig3, mean, cov, alpha=.5, color=color)\n",
    "\n",
    "\n",
    "#show(gridplot([[fig1, fig2], [None, fig3]]))\n",
    "show(gridplot([[fig2, fig3]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
