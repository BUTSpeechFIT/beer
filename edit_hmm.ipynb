{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import beer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welch_forward_test(init_states, trans_mat, lhs):\n",
    "    init_prob = 1 / len(init_states)\n",
    "    alphas = torch.zeros_like(lhs)\n",
    "    scale_factors = torch.zeros(len(lhs)).type(lhs.type())\n",
    "    obsev = lhs[0, init_states] * init_prob\n",
    "    scale_factors[0] = obsev.sum()\n",
    "    alphas[0, init_states] = obsev / scale_factors[0]\n",
    "    for i in range(1, lhs.shape[0]):\n",
    "        obsev = lhs[i] * (trans_mat.t() @ alphas[i-1])\n",
    "        scale_factors[i] = obsev.sum()\n",
    "        alphas[i] = obsev / scale_factors[i]        \n",
    "    return alphas.log(), scale_factors\n",
    "\n",
    "def baum_welch_backward_test(final_states, trans_mat, lhs, scale_factors):\n",
    "    final_prob = 1 / len(final_states)\n",
    "    betas = torch.zeros_like(lhs)\n",
    "    betas[-1, final_states] = final_prob\n",
    "    \n",
    "    for i in reversed(range(lhs.shape[0]-1)):\n",
    "        obsev = trans_mat @ (lhs[i+1] * betas[i+1])\n",
    "        betas[i] = obsev / scale_factors[i+1]\n",
    "    return betas.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tensor(tensor):\n",
    "    idx = torch.arange(len(tensor) - 1, -1, -1, dtype=torch.long)\n",
    "    return tensor[idx]\n",
    "\n",
    "def baum_welch_forward_backward_test(init_states, final_states, trans_mat, llhs):\n",
    "    log_scale_factor = llhs.sum()\n",
    "    scaled_llhs = llhs / log_scale_factor\n",
    "    scaled_llhs = torch.exp(llhs - beer.logsumexp(scaled_llhs, dim=0)) + 1e-6\n",
    "    lhs = torch.exp(scaled_llhs)\n",
    "    log_alphas, scale_factors = baum_welch_forward_test(init_states, trans_mat, lhs)\n",
    "    log_betas = baum_welch_backward_test(final_states, trans_mat, lhs, scale_factors)\n",
    "    \n",
    "    scale_alpha = torch.cumsum(scale_factors.log(), dim=-1)\n",
    "    scale_beta = torch.zeros_like(scale_alpha)\n",
    "    scale_beta[:-1] = reverse_tensor(torch.cumsum(reverse_tensor((scale_factors.log())), dim=-1))[1:]\n",
    "    log_alphas += (scale_alpha)[:, None]\n",
    "    log_betas += scale_beta[:, None]\n",
    "    \n",
    "    return log_alphas, log_betas   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.load('./recipes/timit/data/train_10utt/feats.npz')\n",
    "labs = np.load('./recipes/timit/data/train_10utt/phones.int.npz')\n",
    "keys = list(feats.keys())\n",
    "#with open('./recipes/timit/exp/emission.mdl', 'rb') as m:\n",
    "#    normals = pickle.load(m)\n",
    "    \n",
    "normals = beer.NormalDiagonalCovarianceSet.create(torch.zeros(13), torch.ones(13), ncomp=117, noise_std=10)\n",
    "\n",
    "ft = torch.cat([torch.from_numpy(feats[keys[0]]).float()] * 1) \n",
    "lab = labs[keys[0]]\n",
    "init_states = torch.tensor([0])\n",
    "final_states = torch.tensor([len(lab) - 1])\n",
    "trans_mat = beer.HMM.create_ali_trans_mat(len(lab)).double()\n",
    "aliset = beer.AlignModelSet(normals, lab)\n",
    "hmm = beer.HMM.create(init_states, final_states, trans_mat, aliset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_s_stats = aliset.sufficient_statistics(ft)\n",
    "pc_exp_llh = aliset(len_s_stats).double() * 5\n",
    "print(pc_exp_llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_alphas_1 = beer.HMM.baum_welch_forward(init_states, trans_mat, pc_exp_llh)\n",
    "log_beta_1 = beer.HMM.baum_welch_backward(final_states, trans_mat, pc_exp_llh)\n",
    "log_alphas_2, log_beta_2 = baum_welch_forward_backward_test(init_states, final_states, trans_mat, pc_exp_llh)\n",
    "a1 = log_alphas_1.numpy()\n",
    "a2 = log_alphas_2.numpy()\n",
    "b1 = log_beta_1.numpy()\n",
    "b2 = log_beta_2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(a1+b1, a2+b2)\n",
    "print(a1+b1, '\\n')\n",
    "print(a2+b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (a1+b1).argsort(axis=1)\n",
    "d = np.exp(a2+b2).argsort(axis=1)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c, '\\n')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
